{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Understanding Data Science Classification Metrics in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "In this tutorial, we will walk through a few classifications metrics in Python's scikit-learn and write our own functions from scratch to understand the math behind a few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "One major area of predictive modeling in data science is classification. Classification consists of trying to predict which class a particular sample from a population comes from. \n",
    "\n",
    "\n",
    "\n",
    "For example, if we are trying to predict if a particular patient will be re-hospitalized, the two possible classes are hospitalized (positive) and not-hospitalized (negative). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The classification model then tries to predict if each patient will be hospitalized or not hospitalized. In other words, classification is simply trying to predict which bucket (predicted positive vs predicted negative) a particular sample from the population should be placed as seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<img src=\"img/classification.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The above sketch shows the following: A population consisting of several observations each belonging to either the positive or negative class. The positives are the hospitalized, the negative are the not hospitalized. The observations are used to build a model that aims at predicting whether an observation belongs to the positive or the negative class. As models are not working perfectly in general, there are also negatives among those predicted positive and positives among those predicted negative. End of sketch description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "As you train your classification predictive model, you will want to assess how good it is. \n",
    "\n",
    "Interestingly, there are many different ways of evaluating the performance. Most data scientists that use Python for predictive modeling use the Python package called scikit-learn. Scikit-learn contains many built-in functions for analyzing the performance of models. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here, we will walk through a few of these metrics and write our own functions from scratch to understand the math behind some of them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We will cover the following metrics functions from `sklearn.metrics` :\n",
    "\n",
    "    - confusion_matrix\n",
    "    - accuracy_score\n",
    "    - recall_score\n",
    "    - precision_score\n",
    "    - f1_score\n",
    "    - roc_curve\n",
    "    - roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Here we will write our own functions from scratch assuming a two-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Let's load a sample data set that has the actual labels (actual_label) and the prediction probabilities for two models (model_RF and model_LR). Here the probabilities are the probability of being class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:03.954257Z",
     "start_time": "2020-02-10T23:26:03.910187Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "In most data science projects, you will define a threshold to define which prediction probabilities are labeled as predicted positive vs predicted negative. \n",
    "\n",
    "For now let's assume the threshold is 0.5. Let's add two additional columns that convert the probabilities to predicted labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:06.264280Z",
     "start_time": "2020-02-10T23:26:06.245236Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "df['predicted_RF'] = (df.model_RF >= 0.5).astype('int')\n",
    "df['predicted_LR'] = (df.model_LR >= 0.5).astype('int')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Given an actual label and a predicted label, the first thing we can do is divide our samples in 4 buckets:\n",
    "\n",
    "    - True positive: actual = 1, predicted = 1\n",
    "    - False positive: actual = 0, predicted = 1\n",
    "    - False negative: actual = 1, predicted = 0\n",
    "    - True negative: actual = 0, predicted = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "These buckets can be represented with the following image (original source https://en.wikipedia.org/wiki/Precision_and_recall#/media/File:Precisionrecall.svg) and we will reference this image in many of the calculations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<img src=\"img/buckets.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The above figure describes four possibilites of predictions. It shows a rectangle divided vertically into two halves. Furthermore a centered circle encapsulates a subregion of the left and of the right rectangle. The complete circle encapsulates all observations which have been predicted as positive. The left half of the circle represents the *true* positves while the right half of the circle represents the *false* positives. The region outside of the circle and inside the rectangle represent the false negatives (left halve rectangle) or true negatives (right halve rectangle), resp. These four fields are refered to in subsequent images. Let's call this figure rectangle-circle figure in this notebook. End of figure description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "These buckets can also be displayed using a confusion matrix as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<img src=\"img/conf_matrix.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The figure shows the so called confusion matrix consisting of two rows and two colums. The rows refer to the actual classes while the columns refer to the predictions. The upper and lower row represent negative and positive *observations*, resp. The left and right column refer to negative and positive *predictions*, resp. The four matrix fields contain the number of predicted true negatives, false positives, false negatives and true positives (upper left to lower right). End of figure description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We can obtain the confusion matrix (as a 2x2 array) from scikit learn, which takes as inputs the actual labels and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:09.154276Z",
     "start_time": "2020-02-10T23:26:08.601586Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "where there were 5047 true positives, 2360 false positives, 2832 false negatives and 5519 true negatives. Let's define our own functions to verify `confusion_matrix`. \n",
    "\n",
    "Note that I filled in the first one and you need to fill in the other 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:09.220938Z",
     "start_time": "2020-02-10T23:26:09.182723Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def find_TP(y_true, y_pred):\n",
    "    # counts the number of true positives (y_true = 1, y_pred = 1)\n",
    "    return sum((y_true == 1) & (y_pred == 1))\n",
    "\n",
    "def find_FN(y_true, y_pred):\n",
    "    # counts the number of false negatives (y_true = 1, y_pred = 0)\n",
    "    return # your code here\n",
    "\n",
    "def find_FP(y_true, y_pred):\n",
    "    # counts the number of false positives (y_true = 0, y_pred = 1)\n",
    "    return # your code here\n",
    "\n",
    "def find_TN(y_true, y_pred):\n",
    "    # counts the number of true negatives (y_true = 0, y_pred = 0)\n",
    "    return # your code here\n",
    "\n",
    "print('TP:',find_TP(df.actual_label.values, df.predicted_RF.values))\n",
    "print('FN:',find_FN(df.actual_label.values, df.predicted_RF.values))\n",
    "print('FP:',find_FP(df.actual_label.values, df.predicted_RF.values))\n",
    "print('TN:',find_TN(df.actual_label.values, df.predicted_RF.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Let's write a function that will calculate all four of these for us, and another function to duplicate `confusion_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:09.465149Z",
     "start_time": "2020-02-10T23:26:09.458588Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_conf_matrix_values(y_true,y_pred):\n",
    "    '''calculate TP, FN, FP, TN'''\n",
    "    TP = find_TP(y_true,y_pred)\n",
    "    FN = find_FN(y_true,y_pred)\n",
    "    FP = find_FP(y_true,y_pred)\n",
    "    TN = find_TN(y_true,y_pred)\n",
    "    return TP,FN,FP,TN\n",
    "\n",
    "def my_confusion_matrix(y_true, y_pred):\n",
    "    '''display our own confusion matrix'''\n",
    "    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)\n",
    "    return np.array([[TN,FP],[FN,TP]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:09.797361Z",
     "start_time": "2020-02-10T23:26:09.758610Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "my_confusion_matrix(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Let's verify that our functions worked using Python's built in `assert` and NumPy's `array_equal` functions.\n",
    "\n",
    "- The assert keyword is used when debugging code; lets you test if a condition in your code returns True, if not, the program will raise an AssertionError. [Source](https://www.w3schools.com/python/ref_keyword_assert.asp).\n",
    "- `array_equal`: True if two arrays have the same shape and elements, False otherwise. [Source](https://numpy.org/doc/stable/reference/generated/numpy.array_equal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:10.215611Z",
     "start_time": "2020-02-10T23:26:10.127851Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "assert  np.array_equal(my_confusion_matrix(df.actual_label.values, df.predicted_RF.values),\\\n",
    "                       confusion_matrix(df.actual_label.values, df.predicted_RF.values) ), 'my_confusion_matrix() is not correct for RF'\n",
    "\n",
    "assert  np.array_equal(my_confusion_matrix(df.actual_label.values, df.predicted_LR.values),\\\n",
    "                       confusion_matrix(df.actual_label.values, df.predicted_LR.values) ), 'my_confusion_matrix() is not correct for LR'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Given these four buckets (TP, FP, FN, TN), we can calculate many other performance metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The most common metric for classification is accuracy, which is the fraction of samples predicted correctly as shown below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<img src=\"img/accuracy.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "source": [
    "The figure contains a formula of the definition of accuracy which is the fraction predicted correctly. It is calculated as the sum of true positive and true negatives (numerator) divided by the sum of all predictions being the sum of true positives, true negatives, false positives, and false negatives (denominator). The fraction is also shown according to the fields of the rectangle-circle figure: The numerator is the sum of the left halve of the circle and the right rectangle without the right halve circle. The denominator is the sum of all four fields. End of figure description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We can obtain the accuracy score from scikit learn, which takes as inputs the actual labels and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:11.694189Z",
     "start_time": "2020-02-10T23:26:11.686144Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Define your own function that duplicates `accuracy_score`, using the formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:12.166327Z",
     "start_time": "2020-02-10T23:26:12.162552Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def my_accuracy_score(y_true, y_pred):\n",
    "    '''own function for the metric accuracy'''\n",
    "    # calculates the fraction of samples predicted correctly\n",
    "    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)  \n",
    "    return # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:12.450397Z",
     "start_time": "2020-02-10T23:26:12.398277Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "assert my_accuracy_score(df.actual_label.values, df.predicted_RF.values) == accuracy_score(df.actual_label.values, df.predicted_RF.values), 'my_accuracy_score failed on RF'\n",
    "assert my_accuracy_score(df.actual_label.values, df.predicted_LR.values) == accuracy_score(df.actual_label.values, df.predicted_LR.values), 'my_accuracy_score failed on LR'\n",
    "print('Accuracy RF: %.3f'%(my_accuracy_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Accuracy LR: %.3f'%(my_accuracy_score(df.actual_label.values, df.predicted_LR.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Using accuracy as a performance metric, the RF model is more accurate than the LR model. \n",
    "\n",
    "So should we stop here and say RF model is the best model? No! \n",
    "\n",
    "Accuracy is not always the best metric to use to assess classification models. \n",
    "\n",
    "For example, let's say that we are trying to predict something that only happens 1 out of 100 times. We could build a model that gets 99% accuracy by saying the event never happened. However, we catch 0% of the events we care about. \n",
    "\n",
    "The 0% measure here is another performance metric known as recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Recall (also known as sensitivity) is the fraction of positives events that you predicted correctly as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<img src=\"img/recall.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The image contains a formula of the definition of recall which is the fraction of positives predicted correctly. It is calculated as the number of true positives (numerator) divided by the sum of true positives and false negatives (denominator). The fraction is also shown according to the fields of the rectangle-circle figure: The numerator contains the left halve of the circle. The denominator consists of the left rectangle including the left halve circle. End of figure description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We can obtain the recall score from scikit-learn, which takes as inputs the actual labels and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:14.083445Z",
     "start_time": "2020-02-10T23:26:14.072381Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Define your own function that duplicates `recall_score`, using the formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:14.686070Z",
     "start_time": "2020-02-10T23:26:14.681726Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def my_recall_score(y_true, y_pred):\n",
    "    '''own function for the metric recall'''\n",
    "    # calculates the fraction of positive samples predicted correctly\n",
    "    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)  \n",
    "    return # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:14.970566Z",
     "start_time": "2020-02-10T23:26:14.917211Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "assert my_recall_score(df.actual_label.values, df.predicted_RF.values) == recall_score(df.actual_label.values, df.predicted_RF.values), 'my_accuracy_score failed on RF'\n",
    "assert my_recall_score(df.actual_label.values, df.predicted_LR.values) == recall_score(df.actual_label.values, df.predicted_LR.values), 'my_accuracy_score failed on LR'\n",
    "print('Recall RF: %.3f'%(my_recall_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Recall LR: %.3f'%(my_recall_score(df.actual_label.values, df.predicted_LR.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "One method to boost the recall is to increase the number of samples that you define as predicted positive by lowering the threshold for predicted positive. Unfortunately, this will also increase the number of false positives. \n",
    "\n",
    "Another performance metric called precision takes this into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Precision is the fraction of predicted positives events that are actually positive as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<img src=\"img/precision.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The figure contains a formula for the definition of precision which is the fraction of predicted positives that are actually positive. It is calculated as the number of true positives (numerator) divided by the sum of true positives and false positives (denominator). The fraction is also shown according to the fields of the rectangle-circle figure: The numerator contains the left halve of the circle. The denominator consists of the complete circle. End of figure description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We can obtain the accuracy score from scikit-learn, which takes as inputs the actual labels and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:16.361717Z",
     "start_time": "2020-02-10T23:26:16.349479Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Define your own function that duplicates `precision_score`, using the formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:16.822115Z",
     "start_time": "2020-02-10T23:26:16.765939Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def my_precision_score(y_true, y_pred):\n",
    "    '''own function for the metric precision'''\n",
    "    # calculates the fraction of predicted positives samples that are actually positive\n",
    "    TP,FN,FP,TN = find_conf_matrix_values(y_true,y_pred)  \n",
    "    return # your code here\n",
    "\n",
    "assert my_precision_score(df.actual_label.values, df.predicted_RF.values) == precision_score(df.actual_label.values, df.predicted_RF.values), 'my_accuracy_score failed on RF'\n",
    "assert my_precision_score(df.actual_label.values, df.predicted_LR.values) == precision_score(df.actual_label.values, df.predicted_LR.values), 'my_accuracy_score failed on LR'\n",
    "print('Precision RF: %.3f'%(my_precision_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Precision LR: %.3f'%(my_precision_score(df.actual_label.values, df.predicted_LR.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "In this case, it looks like RF model is better at both recall and precision. But what would you do if one model was better at recall and the other was better at precision. \n",
    "\n",
    "One method that some data scientists use is called the F1 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The f1 score is the harmonic mean of recall and precision, with a higher score as a better model. The f1 score is calculated using the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<img src=\"img/f1_score.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The figure above shows a formula of the F1 score. It can be calculated as 2 (numerator) divided by the denomiator consisting of the sum of two fractions, viz. the sum of the reciprocals of precision and recall. It can also be calculated as 2 times the product of precision and recall (numerator) divided by the sum of precision and recall (denomiator). End of figure description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We can obtain the f1 score from scikit-learn, which takes as inputs the actual labels and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:17.995319Z",
     "start_time": "2020-02-10T23:26:17.982266Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(df.actual_label.values, df.predicted_RF.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Define your own function that duplicates `f1_score`, using the formula above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:18.534616Z",
     "start_time": "2020-02-10T23:26:18.449399Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def my_f1_score(y_true, y_pred):\n",
    "    '''own function for the metric f1 score'''\n",
    "    # calculates the F1 score\n",
    "    recall = my_recall_score(y_true,y_pred)  \n",
    "    precision = my_precision_score(y_true,y_pred)  \n",
    "    return # your code here\n",
    "\n",
    "assert my_f1_score(df.actual_label.values, df.predicted_RF.values) == f1_score(df.actual_label.values, df.predicted_RF.values), 'my_accuracy_score failed on RF'\n",
    "assert my_f1_score(df.actual_label.values, df.predicted_LR.values) == f1_score(df.actual_label.values, df.predicted_LR.values), 'my_accuracy_score failed on LR'\n",
    "print('F1 RF: %.3f'%(my_f1_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('F1 LR: %.3f'%(my_f1_score(df.actual_label.values, df.predicted_LR.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "So far, we have assumed that we defined a threshold of 0.5 for selecting which samples are predicted as positive. If we change this threshold the performance metrics will change. As shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:18.954077Z",
     "start_time": "2020-02-10T23:26:18.904127Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "print('scores with threshold = 0.5')\n",
    "print('Accuracy RF: %.3f'%(my_accuracy_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Recall RF: %.3f'%(my_recall_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('Precision RF: %.3f'%(my_precision_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print('F1 RF: %.3f'%(my_f1_score(df.actual_label.values, df.predicted_RF.values)))\n",
    "print(' ')\n",
    "print('scores with threshold = 0.25')\n",
    "print('Accuracy RF: %.3f'%(my_accuracy_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "print('Recall RF: %.3f'%(my_recall_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "print('Precision RF: %.3f'%(my_precision_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "print('F1 RF: %.3f'%(my_f1_score(df.actual_label.values, (df.model_RF >= 0.25).astype('int').values)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "How do we assess a model if we haven't picked a threshold? One very common method is using the receiver operating characteristic (ROC) curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# roc_curve and roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "ROC curves are VERY help with understanding the balance between true-positive rate and false positive rates. Sci-kit learn has built in functions for ROC curves and for analyzing them. The inputs to these functions (`roc_curve` and `roc_auc_score`) are the actual labels and the predicted probabilities (not the predicted labels).  Both `roc_curve` and `roc_auc_score` are both complicated functions, so we will not have you write these functions from scratch. Instead, we will show you how to use sci-kit learn's functions and explain the key points. Let's begin by using `roc_curve` to make the ROC plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:20.072192Z",
     "start_time": "2020-02-10T23:26:20.058038Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_RF, tpr_RF, thresholds_RF = roc_curve(df.actual_label.values, df.model_RF.values)\n",
    "fpr_LR, tpr_LR, thresholds_LR = roc_curve(df.actual_label.values, df.model_LR.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The `roc_curve` function returns three lists:\n",
    "\n",
    "    - thresholds = all unique prediction probabilities in descending order\n",
    "    - fpr = the false positive rate (FP / (FP+TN)) for each threshold\n",
    "    - tpr = the true positive rate  (TP / (TP+FN)) (i.e. recall) for each threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:20.596378Z",
     "start_time": "2020-02-10T23:26:20.590797Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "thresholds_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:20.833091Z",
     "start_time": "2020-02-10T23:26:20.828987Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "fpr_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:21.152957Z",
     "start_time": "2020-02-10T23:26:21.147431Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "tpr_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We can plot the ROC curve for each model as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:22.036097Z",
     "start_time": "2020-02-10T23:26:21.619413Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(fpr_RF, tpr_RF,'r-',label = 'RF')\n",
    "plt.plot(fpr_LR,tpr_LR,'b-', label= 'LR')\n",
    "plt.plot([0,1],[0,1],'k-',label='random')\n",
    "plt.plot([0,0,1,1],[0,1,1,1],'g-',label='perfect')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "There are a couple things that we can observe from this figure. \n",
    " - A model that randomly guesses the label will result in the black line and you want to have a model that has a curve above this black line. \n",
    "- An ROC that is farther away from the black line is better, so RF (red) looks better than LR (blue).\n",
    "- Although not seen directly, a low threshold results in a point in the top right and a high threshold results in a point in the bottom left. This means as you decrease the threshold you get higher TPR at a cost of higher FPR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "To analyze the performance, we will use the area-under-curve metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:22.678184Z",
     "start_time": "2020-02-10T23:26:22.658071Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_RF = roc_auc_score(df.actual_label.values, df.model_RF.values)\n",
    "auc_LR = roc_auc_score(df.actual_label.values, df.model_LR.values)\n",
    "\n",
    "print('AUC RF:%.3f'% auc_RF)\n",
    "print('AUC LR:%.3f'% auc_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "As you can see, the area under the curve for the RF model is better than the LR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "When I plot the ROC curve, I like to add the AUC to the legend as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:26:23.581755Z",
     "start_time": "2020-02-10T23:26:23.423550Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(fpr_RF, tpr_RF,'r-',label = 'RF AUC: %.3f'%auc_RF)\n",
    "plt.plot(fpr_LR,tpr_LR,'b-', label= 'LR AUC: %.3f'%auc_LR)\n",
    "plt.plot([0,1],[0,1],'k-',label='random')\n",
    "plt.plot([0,0,1,1],[0,1,1,1],'g-',label='perfect')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Overall, in this toy example the model RF wins with every performance metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "In predictive analytics, when deciding between two models it is important to pick a single performance metric. \n",
    "\n",
    "As you can see here, there are many that you can choose from (accuracy, recall, precision, f1-score, AUC, etc). \n",
    "\n",
    "Ultimately, you should use the performance metric that is most suitable for the business problem at hand.\n",
    "\n",
    "Many data scientists prefer to use the AUC because it does not require selecting a threshold and helps balance true positive rate and false positive rate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "13bd8538de40f45cc725a161940eef1142a59efb8815cd2aeede5353e0aed6ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
