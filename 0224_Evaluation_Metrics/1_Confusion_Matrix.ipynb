{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix quick Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates how to create a confusion matrix on a predicted model.\n",
    "\n",
    "\n",
    "For this, we have to import the `confusion_matrix` module from the `sklearn.metrics` library which helps us to generate the confusion matrix. We'll also have a look at the `accuracy_score` and the `classification_report`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T23:23:11.523092Z",
     "start_time": "2020-02-10T23:23:10.893007Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within a confusion matrix, we are comparing the actual target values (y_test) with the values predicted by our model (often called y_pred)\n",
    "# Here, we are creating random values for our actual and predicted values as example\n",
    "actual    = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0] \n",
    "predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0] \n",
    "\n",
    "# Here, we are creating a confusion matrix which compares actual and predicted values\n",
    "results = confusion_matrix(actual, predicted) \n",
    "\n",
    "print ('Confusion Matrix :')\n",
    "print((results) )\n",
    "\n",
    "print ('Accuracy Score :',accuracy_score(actual, predicted) )\n",
    "\n",
    "print ('Report : ')\n",
    "print (classification_report(actual, predicted) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation\n",
    "Take a moment to orient yourself so you don't mix things up:  \n",
    "- In the confusion matrix, what do rows mean and what do columns mean?  \n",
    "- Where are the TP, FP, TN, and FN? \n",
    "> Hint: They are easily identifiable by counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/confusion-matrix-machine-learning/ \n",
    "\n",
    "http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classic Data Science interview question is to ask \"What is better--more false positives, or false negatives?\" \n",
    "\n",
    "\n",
    "This is a trick question designed to test your critical thinking on the topics of precision and recall. \n",
    "\n",
    "\n",
    "\n",
    "As you're probably thinking, the answer is \"It depends on the problem!\". \n",
    "\n",
    "\n",
    "\n",
    "Sometimes, our model may be focused on a problem where False Positives are much worse than False Negatives, or vice versa. For instance, detecting credit card fraud. A False Positive would be when our model flags a transaction as fraudulent, and it isn't. This results in a slightly annoyed customer. On the other hand, a False Negative might be a fraudulent transaction that the company mistakenly lets through as normal consumer behavior. In this case, the credit card company could be on the hook for reimbursing the customer for thousands of dollars because they missed the signs that the transaction was fraudulent! Although being wrong is never ideal, it makes sense that credit card companies tend to build their models to be a bit too sensitive, because having a high recall saves them more money than having a high precision score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a few minutes and see if you can think of at least 2 examples each of situations where a high precision might be preferable to high recall, and 2 examples where high recall might be preferable to high precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "13bd8538de40f45cc725a161940eef1142a59efb8815cd2aeede5353e0aed6ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
