{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier Word Embeddings\n",
    "\n",
    "Let's have a look at word embeddings. Therefore, we want to \n",
    "\n",
    "1. Apply the [TensorFlow tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings) to the Spam/Ham dataset.\n",
    "\n",
    "2. Load the embedings in the [Embedding Projector](http://projector.tensorflow.org/).\n",
    "\n",
    "In this notebook you will find some help how to load and prepare the Spam/Ham data to apply those NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all needed libraries and functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Load the Spam and Ham data with pandas and afterwards split the data into a train, a validation and a test set as usual with sklearns train_test_split function.\n",
    "\n",
    "After spliting the data, we need to transform the data to tensorflow \"tensors\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spam/ham data\n",
    "\n",
    "# Encoding target variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data in train, validation and test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94bc3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data to tf.dataset\n",
    "# try it with tf.data.Dataset.from_tensor_slices, where you specify the text and the target separately.\n",
    "# one solution is shown here: https://medium.com/when-i-work-data/converting-a-pandas-dataframe-into-a-tensorflow-dataset-752f3783c168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "follow the steps in the tutorial, our dataset has a vocab_size of 7546."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a classification model\n",
    "\n",
    "In the tutorial the batch-size was defined when loading the data as tf.Dataset. That's why we have to specify this now too. This is especially important for training the model.\n",
    "You can create the batches as shown here: [tf.data.Dataset.batch() method, combined with repeat() method](https://www.gcptutorials.com/article/how-to-use-batch-method-in-tensorflow).\n",
    "Specify the `batch_size`, for example take 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batched datasets for training, validation and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compiling using Adam optimizer and BinaryCrossentropy loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the loss and accuracy on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving information summary about the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the trained word embeddings and save them to disk\n",
    "\n",
    "Follow the instructions in the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the embeddings\n",
    "To visualize the embeddings, upload them to the embedding projector.\n",
    "\n",
    "Open the [Embedding Projector](http://projector.tensorflow.org/) (this can also run in a local TensorBoard instance).\n",
    "\n",
    "Click on \"Load data\".\n",
    "\n",
    "Upload the two files you created above: vecs.tsv and meta.tsv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.nlp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fb4e7a82a71b285d1d2fdbccfc7dcb00fac1863548a2573de3cd7a5b08c832d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
