{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Gradient Descent\n",
    "\n",
    "We haven't touched classification (*coming up in the next days*) yet, but we can use ```Gradient Descent``` also for classification. In this notebook you can have a look on how to do it and can come back to it later to compare it with the other methods. We will also use this example to visualize SGD and Mini-Batch GD."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of Stochastic Gradient Descent (SGD) and Mini Batch Gradient Descent\n",
    "Now we are looking at examples for the visualisation of the Stochastic Gradient Descent (SGD) and the Mini Batch Gradient Descent using the scikit-learn ```sklearn.linear_model.SGDClassifier()```. First we are loading the digits dataset which consists of 8x8 images of a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as dt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits dataset with two classes\n",
    "digits, target = dt.load_digits(n_class=2, return_X_y=True)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=10, figsize=(12,4), subplot_kw=dict(xticks=[], yticks=[]))\n",
    "\n",
    "# Plot some images of digits\n",
    "for i in np.arange(10):\n",
    "    ax[i].imshow(digits[i,:].reshape(8,8), cmap=plt.cm.gray)   \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are splitting the dataset into train- and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                        digits, target, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = mystdout = StringIO()\n",
    "\n",
    "# You can look up the documentation and play with the hyperparameters\n",
    "clf = SGDClassifier(loss='log', penalty=\"l1\", alpha= 0.1, verbose=1, n_jobs=-1, random_state=10)\n",
    "clf.fit(X_train_sc, y_train)\n",
    "\n",
    "# Plotting the loss function\n",
    "sys.stdout = old_stdout\n",
    "loss_history = mystdout.getvalue()\n",
    "loss_list = []\n",
    "for line in loss_history.split('\\n'):\n",
    "    if(len(line.split(\"loss: \")) == 1):\n",
    "        continue\n",
    "    loss_list.append(float(line.split(\"loss: \")[-1]))\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(loss_list)), loss_list)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = clf.score(X_test_sc, y_test)\n",
    "print(f\"Accuracy: {accuracy.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see with the SGD the model gets better pretty fast. And an accuracy of 97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_stdout = sys.stdout\n",
    "sys.stdout = mystdout = StringIO()\n",
    "\n",
    "# You can look up the documentation and play with the hyperparameters\n",
    "clf = SGDClassifier(loss='log', penalty=\"l1\", alpha= 0.1, verbose=1, n_jobs=-1, random_state=10)\n",
    "\n",
    "\n",
    "\n",
    "# creating the batches N = 6 (the shape of X_train is 288,64) \n",
    "# you can also change the number of batches (keep the shape of X_train in mind)\n",
    "X_train_splits = np.array_split(X_train_sc, 96)\n",
    "y_train_splits = np.array_split(y_train, 96)\n",
    "\n",
    "# training the model on those minibatches\n",
    "for X_train_split, y_train_split in zip(X_train_splits, y_train_splits):\n",
    "    clf.partial_fit(X_train_split, y_train_split, classes= np.unique([1,0]))\n",
    "\n",
    "# Plotting the loss function\n",
    "sys.stdout = old_stdout\n",
    "loss_history = mystdout.getvalue()\n",
    "loss_list = []\n",
    "for line in loss_history.split('\\n'):\n",
    "    if(len(line.split(\"loss: \")) == 1):\n",
    "        continue\n",
    "    loss_list.append(float(line.split(\"loss: \")[-1]))\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(loss_list)), loss_list)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = clf.score(X_test_sc, y_test)\n",
    "print(f\"Accuracy: {accuracy.round(2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to the SGD you can see here that the Mini Batch GD needs seemingly more iterations to get to an similar result of 94% accuracy. But that is due to the lack of early stopping (where the model stops fitting if the loss function doesn't get better) with ```partial_fit()```."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82434c3c60e5606613fe67457fac9471952bc2686da5ed66d9cc97337c01fb00"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
