# Scaling Methods in Machine Learning



**Scaler**            | **Description**                                                                                   | **Pros**                                                                                                                                       | **Cons**                                                                                                    | **When to Use**                                                                                                                                                                                                                                                                                                                                                                              |
|------------------------|---------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [**StandardScaler**](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html)     | Scales data to have mean = 0 and standard deviation = 1.                                          | - Works well with distance-based algorithms.<br>- Symmetrically handles positive and negative values.                                         | - Sensitive to outliers, as mean and standard deviation are affected.                                      | - When data is (approximately) normally distributed.<br>- For SVM, k-NN, PCA, linear regression, and neural networks.                                                                                                                                                                                                                               |
| [**MinMaxScaler**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)       | Scales data to a fixed range, typically [0, 1].                                                   | - Simple and intuitive.<br>- Ideal for algorithms requiring inputs in a fixed range (e.g., neural networks).                                  | - Highly sensitive to outliers since min and max values define the range.                                  | - When data needs to be in a fixed range.<br>- For neural networks or algorithms without assumptions about data distribution.                                                                                                                                                                                                                         |
| [**RobustScaler** ](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.RobustScaler.html)      | Uses median and interquartile range (IQR) to scale data, ignoring outliers.                       | - Insensitive to outliers.<br>- Does not strongly distort the data distribution.                                                              | - Less effective when there are no significant outliers.<br>- May be less precise for narrow distributions. | - When data contains significant outliers.<br>- For robust models like decision trees, random forests, or even SVM and PCA.                                                                                                                                                                                                                          |
| [**MaxAbsScaler**](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)       | Scales data based on the maximum absolute value, ensuring all values fall within \([-1, 1]\).     | - Preserves sparsity of the data.<br>- Works with positive and negative values.<br>- Efficient for large matrices.                            | - Sensitive to extreme maximum values.                                                                     | - For sparse data (e.g., text data or large matrices).<br>- When values need to be constrained to \([-1, 1]\).<br>- For linear models or SVM with sparse inputs.                                                                                                                                                                                                                           |
| [**Normalizer**](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.Normalizer.html)        | Scales each data point (row) to unit length based on the \(L^2\)-norm.                            | - Useful for cosine similarity or algorithms where the direction is more important than magnitude.<br>- Operates row-wise.                    | - Not suitable when absolute values matter (e.g., in regression models).                                   | - For text-based data (e.g., TF-IDF).<br>- For k-Means, k-NN, or models where the direction of data points is relevant.                                                                                                                                                                                                                                |
| [**PowerTransformer**](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.PowerTransformer.html)   | Makes data more Gaussian-like using Yeo-Johnson or Box-Cox transformations.                      | - Reduces skewness.<br>- Stabilizes variance.<br>- Mitigates the impact of outliers.<br>- Automatically optimizes \(\lambda\).                 | - Computationally more intensive.<br>- Unsuitable if data is already close to normal.<br>- Box-Cox: Positive values only. | - For heavily skewed data.<br>- For algorithms assuming normal distribution or homogeneous variance (e.g., linear regression, PCA, Gaussian Naive Bayes).                                                                                                                                                                                                                                   |
| [**QuantileTransformer**](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.QuantileTransformer.html) | Transforms data to a uniform or normal distribution based on quantiles.                          | - Converts any distribution into uniform or Gaussian.<br>- Robust to outliers.                                                                | - Destroys original structure.<br>- May distort relative relationships between data points.                 | - When an algorithm requires uniform or normal distributions.<br>- For models sensitive to skewness or heavy-tailed distributions.                                                                                                                                                                                                                   |
| [**Log Transformation/Function Transformer**](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.FunctionTransformer.html) | Applies logarithm to data, compressing large values.                                             | - Reduces skewness.<br>- Easy to implement.<br>- Compresses large values (e.g., in exponential data).                                         | - Only suitable for positive values.<br>- Cannot handle zero values directly (requires shifting first).     | - For data with exponential or multiplicative relationships.<br>- When large values dominate (e.g., income, population data). 




![alt text](image.png)