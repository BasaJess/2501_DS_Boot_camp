{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG with langchain and Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitation of LLMs\n",
    "\n",
    "- know nothing outside trainning data, e.g. up-to-date information, classified/private data\n",
    "- not specialized in specific use cases\n",
    "- tend to hallucinate confidently, possibly leading to missinformation\n",
    "- produce black box output: do not clarify what has led to the generation of particular content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tunning\n",
    "- Enhances model Performance for specific use case through Transfer Learning with additional data.\n",
    "- Changes model Parameters, enhancing speed and reducing Cost for specific Tasks\n",
    "- Powerful tool for :\n",
    "  - Incorporating non-dynamic or past data.\n",
    "  - Specific Industries with nuances in writting style, and reducing costs for specific Tasks.\n",
    "- Cut-off issue persists in absence of up-to-date Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation\n",
    "- Increases model capabilities through.\n",
    "  - **Retrieving** external and up-to-date information.\n",
    "  - **Augmenting** the original prompt given to the model.\n",
    "  - **Generating** response using context plus Information.\n",
    "- Ground LLM Model parameters remain unchanged (no Transfer Learning).\n",
    "- Powerful Tool for making use of Dynamic up-to-date Information.\n",
    "- White Box Output: Provides transparency behind the Model without Hallucination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Framework\n",
    "<img src=\"../documents/rag.png\" width=\"950\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technolgy Stack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [LangChain](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "> Framework for developing applications powered by LLMs\n",
    "\n",
    "> [What is LangChain? By IBM Technology](https://www.youtube.com/watch?v=1bUy-1hGZpI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [FAISS (Facebook AI Similarity Search)](https://ai.meta.com/tools/faiss/)\n",
    "\n",
    ">  Library allowing storage of contextual embedding vectors in vector database and similarity search\n",
    ">  [acebook AI Similarity Search FAISS | OpenAI's Embeddings Endpoint | Gen AI OpenAI API in Python](https://www.youtube.com/watch?v=Gx3TzYFaCS8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Groq](https://groq.com/about-us/)\n",
    "\n",
    "> Engine providing fast AI inference (conclusion from brand new data) in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Warm up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Prompt template\n",
    "\n",
    "**What is a Prompt?**\n",
    ">- set of instructions or input for an LLM provided by a user to guide its response\n",
    ">- helps it understand the context and generate relevant and coherent language-based output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    given the information {information} about a person I want you to create:\n",
    "    1. A short summary\n",
    "    2. two interesting facts about them\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"information\"],\n",
    "    template=query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Chain\n",
    "**What is a Chain?**\n",
    "\n",
    "> - allows to link the output of one LLM call as the input of another\n",
    "\n",
    "**Note:**\n",
    "The `|` symbol chains together the different components, feeding the output from one component as input into the next component.\n",
    "In this chain the user input is passed to the prompt template, then the prompt template output is passed to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data =\"\"\"\n",
    "Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian computer scientist, cognitive scientist, \n",
    "cognitive psychologist, known for his work on artificial neural networks which earned him the title as the \n",
    "\"Godfather of AI\". Hinton is University Professor Emeritus at the University of Toronto. From 2013 to 2023, \n",
    "he divided his time working for Google (Google Brain) and the University of Toronto, before publicly announcing \n",
    "his departure from Google in May 2023, citing concerns about the risks of artificial intelligence (AI) technology.\n",
    "In 2017, he co-founded and became the chief scientific advisor of the Vector Institute in Toronto.\n",
    "\n",
    "With David Rumelhart and Ronald J. Williams, Hinton was co-author of a highly cited paper published in 1986 \n",
    "that popularised the backpropagation algorithm for training multi-layer neural networks, although they were \n",
    "not the first to propose the approach. Hinton is viewed as a leading figure in the deep learning community.\n",
    "The image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky \n",
    "and Ilya Sutskever for the ImageNet challenge 2012[22] was a breakthrough in the field of computer vision.\n",
    "\n",
    "Hinton received the 2018 Turing Award, often referred to as the \"Nobel Prize of Computing\", together with \n",
    "Yoshua Bengio and Yann LeCun, for their work on deep learning. They are sometimes referred to as the \n",
    "\"Godfathers of Deep Learning\", and have continued to give public talks together. He was also awarded \n",
    "the 2024 Nobel Prize in Physics, shared with John Hopfield.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke(input={\"information\": text_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the information about Geoffrey Everest Hinton:\n",
      "\n",
      "**Summary:** Geoffrey Everest Hinton is a British-Canadian computer scientist, cognitive scientist, and cognitive psychologist known for his work on artificial neural networks and deep learning. He is often referred to as the \"Godfather of AI\" and has made significant contributions to the field of computer vision and machine learning.\n",
      "\n",
      "**Interesting Facts:**\n",
      "\n",
      "1. **Co-author of a highly cited paper:** Hinton co-authored a paper with David Rumelhart and Ronald J. Williams in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks. This paper has had a significant impact on the field of artificial intelligence.\n",
      "2. **Recipient of prestigious awards:** Hinton has received several prestigious awards, including the 2018 Turing Award, often referred to as the \"Nobel Prize of Computing\", and the 2024 Nobel Prize in Physics, shared with John Hopfield. He is also known as the \"Godfather of AI\" and has been recognized as a leading figure in the deep learning community.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Summarazing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf_data(pdf_path):\n",
    "    \"\"\"\n",
    "    this function loads text data from pdf file\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_docs = load_pdf_data(pdf_path = \"../documents/react_paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of loaded pages: 33\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of loaded pages: {len(react_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published as a conference paper at ICLR 2023\n",
      "REAC T: S YNERGIZING REASONING AND ACTING IN\n",
      "LANGUAGE MODELS\n",
      "Shunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\n",
      "1Department of Computer Science, Princeton University\n",
      "2Google Research, Brain team\n",
      "1{shunyuy,karthikn}@princeton.edu\n",
      "2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\n",
      "ABSTRACT\n",
      "While large language models (LLMs) have demonstrated impressive performance\n",
      "across tasks in language understanding and interactive decision making, their\n",
      "abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\n",
      "plan generation) have primarily been studied as separate topics. In this paper, we\n",
      "explore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\n",
      "in an interleaved manner, allowing for greater synergy between the two: reasoning\n",
      "traces help the model induce, track, and update action plans as well as handle\n",
      "exceptions, while actions allow it to interface with and gather additional information\n",
      "from external sources such as knowledge bases or environments. We apply our\n",
      "approach, named ReAct , to a diverse set of language and decision making tasks\n",
      "and demonstrate its effectiveness over state-of-the-art baselines in addition to\n",
      "improved human interpretability and trustworthiness. Concretely, on question\n",
      "answering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\n",
      "issues of hallucination and error propagation in chain-of-thought reasoning by\n",
      "interacting with a simple Wikipedia API, and generating human-like task-solving\n",
      "trajectories that are more interpretable than baselines without reasoning traces.\n",
      "Furthermore, on two interactive decision making benchmarks (ALFWorld and\n",
      "WebShop), ReAct outperforms imitation and reinforcement learning methods by\n",
      "an absolute success rate of 34% and 10% respectively, while being prompted with\n",
      "only one or two in-context examples.\n",
      "1 I NTRODUCTION\n",
      "A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\n",
      "verbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\n",
      "play an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\n",
      "1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\n",
      "sider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\n",
      "reason in language in order to track progress (“now that everything is cut, I should heat up the pot of\n",
      "water”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\n",
      "me use soy sauce and pepper instead”), and to realize when external information is needed (“how do\n",
      "I prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\n",
      "recipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What\n",
      "dish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\n",
      "to learn new tasks quickly and perform robust decision making or reasoning, even under previously\n",
      "unseen circumstances or facing information uncertainties.\n",
      "Recent results have hinted at the possibility of combining verbal reasoning with interactive decision\n",
      "making in autonomous systems. On one hand, properly prompted large language models (LLMs)\n",
      "have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\n",
      "∗Work during Google internship. Projet page with code: https://react-lm.github.io/ .\n",
      "1arXiv:2210.03629v3  [cs.CL]  10 Mar 2023\n"
     ]
    }
   ],
   "source": [
    "print(react_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Document into Chunks\n",
    ">- not possible to feed the whole content into the LLM at once because of finite context window\n",
    ">- even models with large window sizes may struggle to find information in very long inputs and perform very badly\n",
    ">- chunk the document into pieces: helps retrieve only the relevant information from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=800, chunk_overlap=80):\n",
    "    \"\"\"\n",
    "    this function splits documents into chunks of given size and overlap\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents=documents)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_chunks = split_documents(react_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chunks created: 170\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of chunks created: {len(react_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Embeddings\n",
    ">  finding numerical representations of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "def create_embedding_vector_db(chunks, db_name, target_directory=f\"../vector_databases\"):\n",
    "    \"\"\"\n",
    "    this function uses the open-source embedding model HuggingFaceEmbeddings \n",
    "    to create embeddings and store those in a vector database called FAISS, \n",
    "    which allows for efficient similarity search\n",
    "    \"\"\"\n",
    "    # instantiate embedding model\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2'\n",
    "    )\n",
    "    # create the vector store \n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding\n",
    "    )\n",
    "    # save vector database locally\n",
    "    if not os.path.exists(target_directory):\n",
    "        os.makedirs(target_directory)\n",
    "    vectorstore.save_local(f\"{target_directory}/{db_name}_vector_db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
