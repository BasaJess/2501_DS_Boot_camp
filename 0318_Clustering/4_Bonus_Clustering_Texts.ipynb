{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Clustering of Texts\n",
    "\n",
    "Text clustering is widely used in many applications such as recommender systems, sentiment analysis, topic selection or user segmentation. Word embeddings allow to exploit the ordering of words and semantics information from the text corpus. But what are word embeddings exactly? Loosely speaking, they are vector representations of a particular word. Words cannot naively be passed to a neural network (or other machine learning algorithm), they must first be encoded into some kind of numerical form fit for input. We can think of words as being distinct inputs, which nevertheless have some kind of relationship with one another in some kind of abstract space of meanings. For example the words \"princess\" and \"queen\" are close to one another in meaning, while the words \"ball\" and \"square\" are far away.\n",
    "There are a number of different approaches to generating word embeddings, whose relative merit is based on how good they are at placing words in vector space close to one another. The approaches can be split into roughly two categories: probabilistic approaches (e.g. using a neural network to optimize an embedding), and classical \"count-based\" approaches.\n",
    "In this notebook we are going to use two embedding algorithms:\n",
    "\n",
    "- **Word2Vec** is one of the most popular techniques to learn word embeddings using shallow neural networks. It was developed by Tomas Mikolov in 2013 at Google.\n",
    "\n",
    "- **GloVe** is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster import KMeansClusterer\n",
    " \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import confusion_matrix, classification_report, rand_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN, MiniBatchKMeans, KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans in Combination with word2vec\n",
    "\n",
    "We are starting with a simple example using `word2vec`, where we are clustering 10 sentences using `KMeans` Clustering.\n",
    "\n",
    "- 'this', 'is', 'the', 'one','good', 'machine', 'learning', 'book'\n",
    "- 'this', 'is',  'another', 'book'\n",
    "- 'another', 'book', 'in', 'collection'\n",
    "- 'weather', 'rain', 'snow'\n",
    "- 'yesterday', 'weather', 'snow'\n",
    "- 'forecast', 'tomorrow', 'rain', 'snow'\n",
    "- 'tomorrow', 'weather', 'clear'\n",
    "- 'this', 'is', 'the', 'new', 'post'\n",
    "- 'this', 'is', 'about', 'more', 'machine', 'learning', 'post'\n",
    "- 'and', 'this', 'is', 'the', 'one', 'last', 'post', 'book'\n",
    "\n",
    "\n",
    "For the embeddings we will use the gensim word2vec model. With the need to do text clustering at sentence level there will be one extra step for moving from word level to sentence level. For each sentence from the set of sentences, word embedding of each word is summed and in the end divided by the number of words in the sentence. So we are getting the average of all word embeddings for each sentence and use them as we would use embeddings at word level – feeding it to a machine learning clustering algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will cluster our text using the Kmeans algorithm with the word2vec model for embeddings. For this we will use `KMeansClusterer`, the Kmeans implementation from the NLTK library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the sentences as a list \n",
    "sentences = [['this', 'is', 'the', 'one','good', 'machine', 'learning', 'book'],\n",
    "            ['this', 'is',  'another', 'book'],\n",
    "            ['another', 'book', 'in', 'collection'],\n",
    "            ['weather', 'rain', 'snow'],\n",
    "            ['yesterday', 'weather', 'snow'],\n",
    "            ['forecast', 'tomorrow', 'rain', 'snow'],\n",
    "            ['tomorrow', 'weather', 'clear'],\n",
    "            ['this', 'is', 'the', 'new', 'post'],\n",
    "            ['this', 'is', 'about', 'more', 'machine', 'learning', 'post'],  \n",
    "            ['and', 'this', 'is', 'the', 'one', 'last', 'post', 'book']]\n",
    "  \n",
    "\n",
    "# Importing the word2vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    " \n",
    "\n",
    "# Creating a function that calculates the embeddings for the whole sentence by sum up the embedding of \n",
    "# each word und divide it by the sum of words in the sentence\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model.wv[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model.wv[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    return np.asarray(sent_vec) / numw\n",
    "\n",
    "\n",
    "# Saving the embeddings into a list X  \n",
    "X=[]\n",
    "for sentence in sentences:\n",
    "    X.append(sent_vectorizer(sentence, model))   \n",
    "\n",
    "    \n",
    "# Creating the KMeans Clustering model with the implementation of the nltk library\n",
    "NUM_CLUSTERS=2\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=50)\n",
    "assigned_clusters = kclusterer.cluster(X, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you look at our data X you see that it has more than 2 dimensions \n",
    "# In order to plot the data we will use t-SNE to reduce the dimensions to 2\n",
    "model = TSNE(n_components=2, perplexity=5, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    " \n",
    "Y = model.fit_transform(pd.DataFrame(X))\n",
    " \n",
    "# Plotting the results of the clustering\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=assigned_clusters, s=290, alpha=.5)\n",
    " \n",
    "for j in range(len(sentences)):    \n",
    "    plt.annotate(assigned_clusters[j], xy=(Y[j][0], Y[j][1]), xytext=(0,0), textcoords='offset points')\n",
    "    print (\"%s %s\" % (assigned_clusters[j],  sentences[j]))\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data were clustered according to our expectations – sentences were assigned to different clusters by topic. \n",
    "In this example we saw how to use clustering algorithms with word embeddings at sentence level. We used a Kmeans clustering algorithm and a word2vec embedding model. In order to go from word embedding level to sentence embedding level we created a additional function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several Clustering Algorithms in Combination with GloVe\n",
    "\n",
    "`GloVe` stands for Global Vectors for Word Representations and it's a relatively new state of the art natural language processing technique of creating vector space models of word semantics, more commonly known as word embeddings. GloVe is an unsupervised algorithm developed by Stanford University that aims at generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. GloVe is similar to Word2Vec with the primary difference being that Word2Vec is a 'predictive' model that predicts context given a word while GloVe is a count-based model that learns the vectors by essentially doing dimensionality reduction on the co-occurrence counts matrix with respect to minimizing a cost function.\n",
    "\n",
    "We will use 2 categories from the 20 News Groups dataset as our data. The we are going to use and compare `KMeans Clustering`, `MiniBatchKMeans` and `Agglomerative Clustering` as our clustering algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the GLoVe Pre-trained Vectorizer\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the 20 Newsgroups Dataset\n",
    "data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only two Categories (atheism and autos) of the Newsgroup dataset\n",
    "# (Feel free to experiment also with other categories)\n",
    "categories = ['alt.atheism','rec.autos']\n",
    "data = fetch_20newsgroups(categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "sentences = []\n",
    "for sentence in data.data:\n",
    "    # Removing punctuation and special characters\n",
    "    sentence = re.sub(r'[^A-Za-z0-9]+', ' ', sentence)\n",
    "    # Removing the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    sentence = sentence.split()\n",
    "    resultwords  = [word for word in sentence if word.lower() not in stop]\n",
    "    sentences.append(\" \".join(resultwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that calculates the embeddings for the whole sentence by summing up the embedding of \n",
    "# each word und dividing it by the sum of words in the sentence\n",
    "def sent_vectorizer(sent, model):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    values = sent.split()\n",
    "    for w in values:\n",
    "        try:\n",
    "            if numw == 0:\n",
    "                sent_vec = model[w]\n",
    "            else:\n",
    "                sent_vec = np.add(sent_vec, model[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return np.asarray(sent_vec) / numw\n",
    "  \n",
    "# Saving the embeddings in a list X\n",
    "X=[]\n",
    "for sentence in sentences:\n",
    "    X.append(sent_vectorizer(sentence, glove_vectors))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of targets: ', len(data.target))\n",
    "print('Number of Sentences: ', len(sentences))\n",
    "print('Number of X: ', len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA for Dimensionality Reduction\n",
    "# And the StandardScaler to scale the data \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pca_ = PCA(0.99, random_state=0)\n",
    "X_pca=pca_.fit_transform(X_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans\n",
    "\n",
    "First we will use the `KMeans` clustering algorithm. We will look at the `Elbow Method` to have a look how many clusters we should choose. The y-axis of our plot represents the WCSS which is the *\"within-cluster sum of squares\"*. These are the sum of squared distances from observations within a cluster to their centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=150, n_init=10, random_state=0)\n",
    "    kmeans.fit(X_pca)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it is quite difficult to determine the right number of clusters from the plot alone. There are two relatively clear kinks at two and three clusters before the curve flattens, so it makes sense to decide on one of these numbers. In this case we are in a very comfortable situation because we know the right number of clusters, so we choose two as the number we want to work with.\n",
    "\n",
    "Now we will fit our model to the data which earlier was reduced in its dimensionality with a PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the KMeans Clustering model and fit it to our data X_pca after Dimensionality Reduction\n",
    "kmeans = KMeans(n_clusters=2, verbose=0, init='k-means++', max_iter=150, n_init=20, random_state=0)\n",
    "kmeans.fit(X_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the clusters and print our results. Since our data has more than three dimensions we will use t-SNE to reduce the dimensionality to visualize the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using t-SNE to prepare the data for plotting\n",
    "model1 = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "Y = model1.fit_transform(X_pca)\n",
    "\n",
    "plt.scatter(Y[:, 0], Y[:, 1],c=kmeans.labels_,  s=190,alpha=.5); \n",
    "for j in range(len(sentences)):    \n",
    "    plt.annotate(data.target[j], xy=(Y[j][0], Y[j][1]), xytext=(0,0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we know the correct labels we can also evaluate the performance of the algorithm\n",
    "print(confusion_matrix(data.target,kmeans.labels_))\n",
    "print(classification_report(data.target,kmeans.labels_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the classification report the algorithm achieved an **accuracy** of only **8%**. BUT: Because Kmeans knows nothing about the identity of the clusters and assigns the initial centroids randomly, the labels may be permuted. To fix this we will perform the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe with real and predicted labels \n",
    "df_result = pd.DataFrame()\n",
    "df_result['org_target'] = data.target\n",
    "df_result['pred_target'] = kmeans.labels_\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch 0s and 1s for predicted label\n",
    "df_result['pred_target'].replace({1:0, 0:1}, inplace=True)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics with fixed labels\n",
    "print(confusion_matrix(df_result.org_target,df_result.pred_target))\n",
    "print(classification_report(df_result.org_target,df_result.pred_target))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that looks better! As we can see Kmeans could cluster the texts pretty well with an **accuracy** of **92%**. \n",
    "\n",
    "Changing the labels manually is not always feasible, sometimes you will need an evaluation metric which is not sensitive to the permutation of the labels. One metric to choose in this situation is the [Rand Index](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html). It considers not the labels themselves, but their separation. It measure by some distance metric the similarity between the two label sets (the predicted one and the true labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rand Index\n",
    "print('Rand Index: {}'.format(rand_score(data.target,kmeans.labels_)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatch KMeans\n",
    "\n",
    "The second model we are using is `MiniBatchKMeans`. The MiniBatchKMeans is a variation of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the MiniBatchKMeans Clustering model and fit it to our preprocessed data\n",
    "batch_kmeans = MiniBatchKMeans(n_clusters=2, verbose=0, max_iter=150, n_init=20, random_state=0)\n",
    "batch_kmeans.fit(X_pca)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the clusters and print our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using t-SNE to prepare the data for plotting\n",
    "model1 = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "Y = model1.fit_transform(X_pca)\n",
    "\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=batch_kmeans.labels_,  s=190, alpha=.5);\n",
    "for j in range(len(sentences)):    \n",
    "    plt.annotate(data.target[j], xy=(Y[j][0], Y[j][1]), xytext=(0,0), textcoords='offset points') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(data.target,batch_kmeans.labels_))\n",
    "print(classification_report(data.target,batch_kmeans.labels_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the clustering algorithm has reverse-mapped the class labels to the real labels. We can use the same procedure as above to fix this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframe and change predicted target labels\n",
    "df_result = pd.DataFrame()\n",
    "df_result['org_target'] = data.target\n",
    "df_result['pred_target'] = batch_kmeans.labels_\n",
    "df_result['pred_target'].replace({1:0, 0:1}, inplace=True)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics with fixed labels\n",
    "print(confusion_matrix(df_result.org_target,df_result.pred_target))\n",
    "print(classification_report(df_result.org_target,df_result.pred_target))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see MiniBatchKMeans has an **accuracy** of **93%**. That's nearly the same as Kmeans. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering\n",
    "\n",
    "The third algorithm we are going to use is `AgglomerativeClustering`. The AgglomerativeClustering object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the AgglomerativeClustering model and fit it to our preprocessed data\n",
    "agg_clust = AgglomerativeClustering(n_clusters=2, linkage='ward').fit(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using t-SNE to prepare the data for plotting\n",
    "model1 = TSNE(random_state=0)\n",
    "\n",
    "Y = model1.fit_transform(X_pca)\n",
    "\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=agg_clust.labels_,  s=190, alpha=.5);\n",
    "for j in range(len(sentences)):    \n",
    "    plt.annotate(data.target[j], xy=(Y[j][0], Y[j][1]), xytext=(0,0), textcoords='offset points') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(data.target,agg_clust.labels_))\n",
    "print(classification_report(data.target,agg_clust.labels_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the clustering algorithm has reverse-mapped the class labels to the real labels. We can use the same procedure as above to fix this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataframe and change predicted target labels\n",
    "df_result = pd.DataFrame()\n",
    "df_result['org_target'] = data.target\n",
    "df_result['pred_target'] = agg_clust.labels_\n",
    "df_result['pred_target'].replace({1:0, 0:1}, inplace=True)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the AgglomerativeClustering algorithm \n",
    "print(confusion_matrix(df_result.org_target,df_result.pred_target))\n",
    "print(classification_report(df_result.org_target,df_result.pred_target))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see `AgglomerativeClustering` performed similarly to the other algorithms achieving an **accuracy** of **91%**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN \n",
    "\n",
    "We also tried `DBSCAN` on the data but it has performed relatively poorly.\n",
    "\n",
    "You are welcome to try it yourself:\n",
    "\n",
    "Remember to tweak the hyperparameters of DBSCAN to get different results (you should definitely do this, if all values are considered to be in one cluster only).\n",
    "\n",
    "If you check out the classification report, you will stumble across a `-1` \"cluster\". This is no cluster, but all the points that are considered as noise by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code!\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ee5ce8695951d3743acb1574d7cbc518a435066b16546d59d44a9748127e061"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
