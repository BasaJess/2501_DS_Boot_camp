{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e526c89",
   "metadata": {},
   "source": [
    "# Building a Dense Neural Network\n",
    "In this notebook you will build your own dense neural network for 2-class prediction, implementing\n",
    "1. Model architecture\n",
    "2. Parameter initializaion\n",
    "3. Forward propagation\n",
    "4. Loss function computation\n",
    "5. Backward propagation\n",
    "6. Gradient descent parameter update\n",
    "7. Training algorithm\n",
    "8. Prediction\n",
    "\n",
    "At the end of this notebook you should have learned the following:\n",
    "- An understanding of how a neural network is constructed\n",
    "- How a neural network comes to a prediction\n",
    "- How backward propagation and gradient descent trains a neural network\n",
    "- How hidden layer dimension and learning rate influences the training and/or prediction outcome\n",
    "- How the problem setting defines the performance of a nerual network\n",
    "\n",
    "## Import Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662587fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from dnn_utils import sigmoid, visualize_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the dataset\n",
    "X, Y = datasets.make_moons(10000, noise=0.07, random_state=24 )\n",
    "plt.scatter(X[:,0],X[:,1], c=Y)\n",
    "plt.title('The moon data')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7f3dc",
   "metadata": {},
   "source": [
    "From the plot above, it can be seen clearly that the classes of this dataset cannot be separated by a linear decision boundary, so a logisic regression would do quite bad on it (try it out). \n",
    "\n",
    "Let us take a look at the data dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data dimensions\n",
    "print(' input dimensions: {}'.format(X.shape))\n",
    "print('output dimensions: {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11077060",
   "metadata": {},
   "source": [
    "## Prerequisite: Logistic regression\n",
    "To have an appropriate benchmark model a logistic regression should be fit to the data. Note that a logistic regression possesses a linear decision boundary. For the data shown above this will be problematic. \n",
    "\n",
    ">__Exercise__: Fit a logistic regression model to the data and print out the test accuracy score. Use a 70/30 test split with a random seed of 42. Fit the model with the already imported `LogisticRegressionCV()`. Visualize the decision boundary by using the function `visualize_decision_boundary()` from the `dnn_utils.py`. Note that `visualize_decision_boundary()` expects a model's `predict()` function. This can be passed by using a `lambda` function: `lambda x: model.predict(x)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44660250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========#\n",
    "# YOUR CODE # ~ 4 lines\n",
    "#===========#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a786384",
   "metadata": {},
   "source": [
    "## 1. Model architecture\n",
    "For each layer the number of parameters are defined by their input and ouput, respectively. The dense neural network has the following layers:\n",
    "\n",
    "1. input layer\n",
    "2. hidden layer\n",
    "3. output layer\n",
    "\n",
    "<img src=\"../images/artificial_network_architecture_dnn.png\" alt=\"\" width=\"400\"/>\n",
    "\n",
    "\n",
    "All layers have an input and output. The input layer's input are the number of features `n_input` in `X` and the output layer's output is the number of classes `n_output`. In the case of 2-class output we need only a single output, so `n_output=1`. The interesting part is the one that happens in the hidden layer. The input to this layer is defined by the output of the previous layer, the input layer. The size of the output of the input layer is `n_input` (remember: this is a throughput layer; it just passes through). The output dimension of the hidden layer is defined in the variable `n_hidden`. Together these variables define the size of the weight matrices and biases that need to be learned by the network.\n",
    "\n",
    ">__Exercise__: Define a function that determines the dimensions of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_architecture(X,Y, n_hidden=10):\n",
    "    \"\"\"\n",
    "    Defines the DNN model architecture\n",
    "    \n",
    "        Args:\n",
    "        X: feature vector with shape (n_samples, n_input)\n",
    "        Y: true target of shape (n_samples, 1)   \n",
    "        n_hidden: size of the hidden layer (defined by user)    \n",
    "        \n",
    "        Returns:\n",
    "        arch: model architecture with sizes of layers in a dict\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the layer sizes\n",
    "    #===========#\n",
    "    # YOUR CODE # ~ 3 lines\n",
    "    #===========#\n",
    "    \n",
    "    #===========#\n",
    "    # COMPLETE  # \n",
    "    #===========#    \n",
    "    arch = {'n_input', 'n_hidden', 'n_output'}\n",
    "\n",
    "    return arch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f4901",
   "metadata": {},
   "source": [
    "## 2. Parameter initialization\n",
    "Parameter initialization in neural networks is a delicate issue as in contrast to other machine learning methods we cannot set starting weights to zero (it does work for the bias though). The reason for this is that starting from zero will give uninformative gradients and your model will not improve. \n",
    "\n",
    "There are many different ways of parameter initialization in neural network literature. Some of them are used more often than others. Here we will use random weigh initialization, i.e. we will initialize all weights randomly with mean zero and a certain variance (if too low gradients will again tend to vanish). \n",
    "\n",
    "**_Info_**: Two other initializations worth noting are _He initialization_ and _Xavier initialization_ also named _Glorot initialization_ (especially in multiple software packages).\n",
    "\n",
    "From the mathematical model it should become clear which parameters exist and must be initialized:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z}^{[1]}_i&=\\mathbf{W}^{[1]}\\mathbf{x}_i+\\mathbf{b}^{[1]}\\\\\n",
    "\\mathbf{a}^{[1]}_i&=\\tanh(\\mathbf{z}^{[1]}_i)\\\\\n",
    "\\mathbf{z}^{[2]}_i&=\\mathbf{W}^{[2]}\\mathbf{a}^{[1]}_i+\\mathbf{b}^{[2]}\\\\\n",
    "\\mathbf{a}^{[2]}_i&=\\text{sigmoid}(\\mathbf{z}^{[2]}_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "A threshold then determines which class is assigned to the $i$th instance:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i=\\begin{cases}\n",
    "1&a^{[2]}_i>0.5\\\\\n",
    "0&a^{[2]}_i\\leq0.5\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**_Info_**: The usual way how neural networks of any kind are built is to first define the network architecture (i.e. `n_inputs`, `n_hidden`, and `n_output`). Then step (2.) from [above](#2-parameter-initialization) (parameter initialization) and a loop through steps (3.)-(5.). In many programs you can find some wrapping function that holds steps  (1.)-(5.) and trains your whole network. After training you can use the trained model parameters for prediction.\n",
    "\n",
    ">__Exercise__: Complete the function `init_params` for parameter initialization using random starting parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(n_input,n_hidden,n_output):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "        n_input:  input layer size\n",
    "        n_hidden: hidden layer size\n",
    "        n_output: output layer size\n",
    "        \n",
    "        Returns:\n",
    "        params: dict holding the network parameters:\n",
    "            W1: weight matrix of hidden layer\n",
    "            b1: bias of hidden layer\n",
    "            W2: weight matrix of output layer\n",
    "            b2: bias of output layer\n",
    "    \"\"\"\n",
    "    # Seed serves for output checks\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    #===========#\n",
    "    # YOUR CODE #\n",
    "    #===========#\n",
    "    \n",
    "    assert (W1.shape==(n_hidden, n_input))\n",
    "    assert (b1.shape==(n_hidden, 1))\n",
    "    assert (W2.shape==(n_output, n_hidden))\n",
    "    assert (b2.shape==(n_output, 1))\n",
    "    \n",
    "    #===========#\n",
    "    # COMPLETE  #\n",
    "    #===========#\n",
    "    params = {'W1', 'b1', 'W2', 'b2'}\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34faac5a",
   "metadata": {},
   "source": [
    "## 3. Forward propagation\n",
    "Forward propagation is the step through the network model from input to output. \n",
    "\n",
    ">__Exercise__: Complete the `forward_prop()` function below to compute the forward step through the network. You can use the `sigmoid()` function from the `dnn_utils.py`. \n",
    "\n",
    "**_Info_**: The `forward_prop()` function returns next to the model output a `cache` that holds the layer vectors for the backpropagation step. By caching these values the backpropagation step does not need to recalculate them - this saves computation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013fc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, params):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "        X:      input data (NxD) \n",
    "        params: model parameters as a dictionary\n",
    "        \n",
    "        Returns:\n",
    "        A:     activations of output layer\n",
    "        cache: dict containing activations A and layer output Z                \n",
    "    \"\"\"\n",
    "    \n",
    "    # Forward propagation\n",
    "    \n",
    "    #===========#\n",
    "    # YOUR CODE # ~4 lines\n",
    "    #===========#\n",
    "    \n",
    "    assert(A2.shape==(1, X.shape[0]))\n",
    "    \n",
    "    #===========#\n",
    "    # COMPLETE  # \n",
    "    #===========#    \n",
    "    cache = {'Z1', 'A1', 'Z2', 'A2'}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af0496",
   "metadata": {},
   "source": [
    "## Loss function computation\n",
    "After the forward propagation the network output is used in the loss function to evaluate the model's expressivity. In this case we use the _log-loss_ function (this is actually the 2-class form of the _cross-entropy_):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})=-\\frac{1}{N}\\sum_{i=1}^N\\left(y_i\\log(a^{[2]}_i)+(1-y_i)\\log(1-a^{[2]}_i)\\right)\n",
    "$$\n",
    "\n",
    "where $N$ is the number of samples `n_samples` and $a_i^{[2]}$ denotes the $i$th element of the activation of the output layer, $\\mathbf{a}^{[2]}$.\n",
    "\n",
    "**_Info_**: Using matrix multiplication loss computation can be vectorized and increases speed (do not use a loop). Check out `np.multiply()`. Keep in mind that the loss function returns finally a scalar value. \n",
    "\n",
    ">__Exercise__: Complete the function `logloss()` below to compute the model's loss function used in optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c498a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(A, Y):\n",
    "    \"\"\"\n",
    "    Computes the logloss (cross-entropy)\n",
    "    \n",
    "        Arguments:\n",
    "        A:      output activations\n",
    "        Y:      true target of shape (n_samples,)\n",
    "\n",
    "        Returns:\n",
    "        logloss: cost of the actual parameter choice\n",
    "    \"\"\"\n",
    "    \n",
    "    # Implement here the loss function\n",
    "    #===========#\n",
    "    # YOUR CODE # ~2 lines\n",
    "    #===========#\n",
    "    \n",
    "    # Squeeze the dimensions of the scalar-valued vector\n",
    "    loss = float(np.squeeze(loss))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a46bdd",
   "metadata": {},
   "source": [
    "## Backward propagation \n",
    "The backward propagation propagates the error back into the network by using the chain rule for derivatives. What is needed are the derivatives of the loss function towards each parameter, i.e. all weights and biases. Backpropagation is usually the most demanding part of programming a deep learning network. \n",
    "\n",
    "The following derivatives need to be implemented: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial z^{[2]}_i}&=(a^{[2]}_i-y_i)\\\\\n",
    "\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial\\mathbf{W}^{[2]}}&=\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial z^{[2]}_i}\\mathbf{a}^{[1]T}_i\\\\\n",
    "\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial b^{[2]}}&=\\sum_{i=1}^N\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial z^{[2]}_i}\\\\\n",
    "\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial \\mathbf{z}^{[1]}_i}&=\\mathbf{W}^{[2]T}\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial z^{[2]}_i}*(\\mathbf{1}-(\\mathbf{a}^{[1]}_i)^2)\\\\\n",
    "\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial \\mathbf{W}^{[1]}}&=\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial \\mathbf{z}^{[1]}_i}\\mathbf{x}_i^T\\\\\n",
    "\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial \\mathbf{b}^{[1]}}&=\\sum_{i=1}^N\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial\\mathbf{z}^{[1]}_i}\n",
    "\\end{align}\n",
    "$$\n",
    "where $T$ stands for the transposed of a vector or matrix, respectively, and $\\mathbf{1}$ is a vector of ones. The $*$-operator denotes a convolution, i.e. an elementwise multiplication for vectors and matrices.\n",
    "\n",
    "**_Info_**: You might note that you have to derive also the activation functions $\\phi()=\\{\\tanh,\\text{sigmoid}\\}$ to step back from the activations to the parameters. For $\\phi=\\tanh$, $\\phi^{\\prime}(\\mathbf{z}^{[1]})=1-(\\mathbf{a}^{[1]})^2$. \n",
    "\n",
    ">__Exercise__: Complete the `backward_prop()` function below by using all your skills in matrix algebra. Normalize all parameter gradients with the sample size to get smoother batch gradients. (_Hint_: the derivative of the loss function towards the second layer output, $\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial z^{[2]}_i}=(a^{[2]}_i-y_i)$ in vector form is $A2-Y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1523f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(X, Y, params, cache):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "        X:      feature matrix with shape (n_samples, n_inputs)\n",
    "        Y:      true target of shape (n_samples,1)\n",
    "        params: model parameters as a dictionary\n",
    "        cache:  cached layer outputs (Z1,A1,Z2,A2)\n",
    "        \n",
    "        Returns:\n",
    "        grads: dict of gradients \n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize the parameter gradients with this batch size.\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Backward propagation: \n",
    "    # Compute: dW2, db2, dW1, db1\n",
    "    # These are six equations.\n",
    "    #===========#\n",
    "    # YOUR CODE # ~6 lines\n",
    "    #===========#\n",
    "    \n",
    "    grads = {'dW1', 'db1', 'dW2', 'db2'}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aebfd4c",
   "metadata": {},
   "source": [
    "## 6. Gradient descent parameter update\n",
    "The parameter update in gradient descent for neural networks is actually similar to the general one: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{W}^{[j]}_{t+1}&=\\mathbf{W}^{[j]}_t - \\alpha\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial\\mathbf{W}^{[j]}}\\big|_t\\\\\n",
    "\\mathbf{b}^{[j]}_{t+1}&=\\mathbf{b}^{[j]}_{t} - \\alpha\\frac{\\partial\\mathcal{L}(\\mathbf{y},\\mathbf{a}^{[2]})}{\\partial\\mathbf{b}^{[j]}}\\big|_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate (step-size of Gradient Descent). A learning rate too small increases training time and a learning rate too large lets the algorithm diverge. So the learning rate is a hyperparameter to be cared for. It is one og the most important hyperparameters in optimization of neural networks. In theory the learning rate needs to decrease sufficiently fast to ensure that the Gradient Descent converges.  \n",
    "\n",
    "\n",
    ">__Exercise__: Complete the `update_params()` function below to update the parameters by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad056e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate=1.5):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "        params:        model parameters as a dictionary\n",
    "        grads:         gradients of parameters as a dictionary\n",
    "        learning_rate: gradient descent step-size\n",
    "        \n",
    "        Returns:\n",
    "        params: updated model parameters as a dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Update all parameters\n",
    "    #===========#\n",
    "    # YOUR CODE # ~4 lines\n",
    "    #===========#\n",
    "    \n",
    "    params = {'W1', 'b1', 'w2', 'b2'}\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078bb16a",
   "metadata": {},
   "source": [
    "## 7. Training algorithm\n",
    "The traning algorithm puts all the steps together. Here you build your complete neural network. Gradient descent is used here in _batch_ form, i.e. we use at each update __all__ samples in the data set. The maximum number of iterations defines how often the update is made. \n",
    "\n",
    ">__Exercise__: Complete the function `train_dnn()` below to implement training for your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc641d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dnn(X, Y, n_hidden, max_iter=10e3, learning_rate=1.2, verbose=False):\n",
    "    \"\"\"\n",
    "    Trains a dense neural network with 'n_hidden' hidden layers.\n",
    "    \n",
    "        Args:\n",
    "        X:             feature vector with shape (n_samples,n_input)\n",
    "        Y:             true target of shape (n_samples,1)\n",
    "        n_hideen:      number of hidden layers\n",
    "        learning_rate: gradient descent step-size\n",
    "        max_iter:      maximum number of iterations of gradient descent\n",
    "        verbose:       print info during optimization\n",
    "        \n",
    "        Returns:\n",
    "        params: dict of trained model parameters (can be used for prediction)        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize parameters of the model\n",
    "    #===========#\n",
    "    # YOUR CODE # ~1 line\n",
    "    #===========#\n",
    "    \n",
    "    # Gradient descent \n",
    "    # Loop until iterator is exhausted\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        # 1. Forward propagation\n",
    "        #===========#\n",
    "        # YOUR CODE # ~1 line\n",
    "        #===========#\n",
    "        \n",
    "        # 2. Calculate loss\n",
    "        #===========#\n",
    "        # YOUR CODE # ~1 line\n",
    "        #===========#\n",
    "        \n",
    "        # 3. Backward propagation\n",
    "        #===========#\n",
    "        # YOUR CODE # ~1 line\n",
    "        #===========#\n",
    "        \n",
    "        # 4. Parameter Update\n",
    "        #===========#\n",
    "        # YOUR CODE # ~1 line\n",
    "        #===========#\n",
    "        \n",
    "        # Gradient descent info\n",
    "        if verbose:\n",
    "            if i % 500: \n",
    "                print('Loss after %i iterations: %f' % (i+1, loss))\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28015c3c",
   "metadata": {},
   "source": [
    "## 8. Prediction\n",
    "In prediction the trained parameters are used to calculate a network output and a threshold makes a class assignment. Make sure that you reuse a function you already programmed before. \n",
    "\n",
    ">__Exercise__: Complete the `predict()` function for your neural network. Use a threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, params):\n",
    "    \"\"\"\n",
    "    Predicts targets with trained model \n",
    "    \n",
    "        Args:\n",
    "        X: feature vector with shape (n_samples, n_input)\n",
    "        params: model parameters as a dictionary\n",
    "        \n",
    "        Returns:\n",
    "        preds: model predictions \n",
    "    \"\"\"\n",
    "    \n",
    "    # DNN gives out probabilities \n",
    "    # A threshold defines how results are assigned to classes\n",
    "    #===========#\n",
    "    # YOUR CODE # ~2 lines\n",
    "    #===========#\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values\n",
    "predicted = predict(X_train, params)\n",
    "\n",
    "print('Predictions: ')\n",
    "print('------------')\n",
    "print('Train accuracy: %f' % accuracy_score(y_train, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871bd7e",
   "metadata": {},
   "source": [
    "## Train your neural network on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411134f6",
   "metadata": {},
   "source": [
    ">__Exercise__: Train the dense neural network on the data given above and predict. Use a train/test split of 70/30 and print out the test accuracy. Use the function `visualize_decision_boundary()` from the `dnn_utils.py` to visualize the networks decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========#\n",
    "# YOUR CODE # ~3 lines\n",
    "#===========#\n",
    "print('Training results: ')\n",
    "print('-----------------')\n",
    "print('train acc.: %f' % accuracy_score(y_train, preds_train))\n",
    "print(' test acc.: %f' % accuracy_score(y_test, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa095238",
   "metadata": {},
   "source": [
    "### Tune your neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc937f6",
   "metadata": {},
   "source": [
    ">__Exercise__: Tune your network from the last exercise by changing the hidden layer's dimension. Use a train/val split of 80/20 of the train set from the last exercise and print out the test accuracy for each hyperparameter choice. Use the function `visualize_decision_boundary()` from the `dnn_utils.py` to visualize how good your network becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e67315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========#\n",
    "# YOUR CODE # ~10 lines\n",
    "#===========#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766dd362",
   "metadata": {},
   "source": [
    "### Improve neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adb956",
   "metadata": {},
   "source": [
    ">__Extra Exercise__: Play around with the _learning rate_ of the gradient descent. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0517208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========#\n",
    "# YOUR CODE # \n",
    "#===========#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad227ce",
   "metadata": {},
   "source": [
    "## Further datasets (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbaa1e",
   "metadata": {},
   "source": [
    "The gradient of the weights is also directly obtained from the error via the outer product:>__Extra Exercise__: Try out other datasets listed below and investigate how your dense neural network performs on this data (you will have to train the network anew). \n",
    "\n",
    "Datasets:\n",
    "- `sklearn.datasets.make_blobs()` - keep in mind that your network is only able to handle 2-dimensional features and two classes\n",
    "- `sklearn.datasets.make_circles()`\n",
    "- `sklearn.datasets.make_classifications()` \n",
    "- `sklearn.datasets.make_hastie_10_2()` \n",
    "- `sklearn.datasets.make_gaussian_quantiles()` \n",
    "- you can also try out what happens, if you use random data with no structure at all (but two classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b323d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========#\n",
    "# YOUR CODE #\n",
    "#===========#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f20623b1c8f5131fefe8ffd828144ee4a1d291eabd09c6cf9853c3b4ce8455a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
