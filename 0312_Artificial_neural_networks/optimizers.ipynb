{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "Neural network optimizers are algorithms that adjust a model's weights to minimize the loss function during training. They guide how weights are updated in response to the calculated gradients from backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optimizers](https://miro.medium.com/v2/resize:fit:640/format:webp/1*XVFmo9NxLnwDr3SxzKy-rA.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![optimizers_overview](https://miro.medium.com/v2/resize:fit:640/format:webp/1*SjtKOauOXFVjWRR7iCtHiA.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "- Batch gradient descent\n",
    "- Stochastic gradient descent\n",
    "- Mini-batch gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "* Based on stochastic gradient descent. Full name is SGD with momentum.\n",
    "* Adds a fraction (momentum term) of the previous update to the current update.\n",
    "* Helps escape local minima and smoothens updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![momentum](https://miro.medium.com/v2/resize:fit:720/format:webp/1*L5lNKxAHLPYNc6-Zs4Vscw.png))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp (Root Mean Square Propagation)\n",
    "\n",
    "- Maintain a moving (discounted) average of the square of gradients\n",
    "- Divide the gradient by the root of this average\n",
    "- if the surface is flat --> big jump, if steep --> small jump\n",
    "\n",
    "Suggested Default values by Hinton(developer of RMSProp):\n",
    "- decay factor γ: 0.9 (keeps 90% of the previous gradient information)\n",
    "- learning rate η: 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAM (Adaptive Moment Estimation)\n",
    "ADAM optimizes neural networks by adjusting learning rates for each parameter dynamically. It combines:\n",
    "- *Momentum*: Uses an exponentially moving average of past gradients (first moment estimate).\n",
    "- *RMSprop*: Scales updates based on recent squared gradients (second moment estimate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Comparison Table\n",
    "| Optimizer  | Description | Pros  | Cons |\n",
    "|------------|------------|---------|---------|\n",
    "| **SGD** (Stochastic Gradient Descent) | Basic gradient descent using a small batch of data. | - Simple & efficient <br> - Good generalization | - Slow convergence <br> - High variance in updates |\n",
    "| **SGD + Momentum** | SGD with an additional term to smooth updates. | - Faster than vanilla SGD <br> - Reduces oscillations | - Still needs manual learning rate tuning |\n",
    "| **AdaGrad** | Adapts learning rate per parameter based on past gradients. | - Good for sparse data <br> - No manual learning rate tuning | - Learning rate decreases too much over time |\n",
    "| **AdaDelta** | Improvement over AdaGrad, avoids aggressive learning rate decay. | - No need to set learning rate <br> - Works well in some cases | - Can be computationally expensive |\n",
    "| **RMSprop** | Adapts learning rate based on recent squared gradients. | - Works well with non-stationary objectives <br> - Handles sparse gradients | - Learning rate decay may require tuning |\n",
    "| **ADAM** (Adaptive Moment Estimation) | Combines Momentum & RMSprop for adaptive learning rates. | - Fast convergence <br> - Works well for most tasks | - Can generalize worse than SGD <br> - Higher memory usage |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
