{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Tensorflow/Keras\n",
    "\n",
    "In this notebook we will use a simple DNN for classification. We will use the vehicle emissions dataset provided by the city of Albuquerque [here](https://www.cabq.gov/abq-data/), in order to predict whether a vehicle will fail the emissions test with the goal to reduce pollution in Albuquerque. We borrow a lot from work done [here](https://github.com/Guli-Y/wimlds_emissions).\n",
    "\n",
    "Since it is a large dataset, we have taken a subset of the data. And we'll skip the exploration part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries I need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# ignore Deprecation Warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential # intitialize the ANN\n",
    "from keras.layers import Dense, Activation, Dropout      # create layers\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "df = pd.read_csv('../data/vehicle_emissions.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable Description:**\n",
    "\n",
    "|Variable|Definition   | Key  |  Type |\n",
    "|---|---|---|---|\n",
    "| RESULT | Passed the test? |   0 = Yes, 1 = No | Dichotomous | \n",
    "| VIN | Vehicle Identification Number | Text | 17 Char |\n",
    "| TEST_SDATE | Start of test date/time | MM/DD/YYYY HH:MM:SS | Date/Time |\n",
    "| MODEL_YEAR | Vehicle Model Year |\tNumber | Year |\n",
    "| GVWR | Gross Vehicle Weight | Number | Ratio |\n",
    "| ENGINE_SIZE | Engine Size volume | Number | Ratio |\n",
    "| ODOMETER | Odometer Reading, number of miles | Number | Ratio |\n",
    "| MAKE | Vehicle Make | Text | Nominal |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Looking at the information about each column and the missing values shows us that we have to clean our data before we can use it for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform TEST_SDATE to datetime object\n",
    "df['TEST_SDATE'] = pd.to_datetime(df['TEST_SDATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the vehicle age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineering VEHICLE_AGE\n",
    "df['VEHICLE_AGE'] = df.TEST_SDATE.dt.year.astype('int') - df.MODEL_YEAR.astype('int') + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean the VIN column and fill missing or zero GVMR values, if possible otherwise drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(f'\\nRecords in input data: {df.shape[0]}', 'red'))\n",
    "# get median GVWR for each VIN\n",
    "# clean VIN\n",
    "df['VIN'] = df['VIN'].astype('string').str.strip().str.lower()\n",
    "# get median GVWR\n",
    "tmp = df[['VIN', 'GVWR']].groupby('VIN').agg({'GVWR':'median'})\n",
    "tmp.reset_index(inplace=True)\n",
    "# merge tmp with df\n",
    "df = df.merge(tmp, how='left', on='VIN', suffixes=('_0',''))\n",
    "# replace 0 with np.nan\n",
    "df.loc[df.GVWR==0, 'GVWR'] = np.nan\n",
    "df.loc[df.GVWR_0==0, 'GVWR_0'] = np.nan\n",
    "# using GVWR_0 fill missing values in GVWR\n",
    "df['GVWR'] = df.GVWR.fillna(df.GVWR_0)\n",
    "# keep GVW and drop GVWR_0\n",
    "df = df.drop(columns=['GVWR_0'])\n",
    "# drop na in GVWR\n",
    "print('\\nRecords with missing GVWR:', df.GVWR.isnull().sum())\n",
    "df = df[~df.GVWR.isnull()]\n",
    "# drop low numbers in GVWR\n",
    "df = df[df.GVWR > 1000]\n",
    "print(colored(f'\\nRecords after droping rows where GVWR is < 1000 or missing: {df.shape[0]}', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have independent test results, only keep the first if there are multiple in 90 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a vehicle has multiple test records within 90 days, keep earliest record \n",
    "df = df.sort_values('TEST_SDATE')\n",
    "df = df.loc[~(df.groupby('VIN')['TEST_SDATE'].diff() < np.timedelta64(90, 'D'))]\n",
    "print(colored(f'\\nRecords after keeping only the earliest test within a month for each vehicle: {df.shape[0]}', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop the `VIN` column as it does not contain useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['VIN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the `ODOMETER` column by removing zeros and `8888888` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 0s in ODOMETER and remove 8888888\n",
    "print('\\nRecords where ODOMETER = 0:', df[df.ODOMETER==0].shape[0])\n",
    "df = df[(df.ODOMETER!=0) & (df.ODOMETER!=8888888)]\n",
    "print(colored(f'\\nRecords after droping rows where ODOMETER is missing: {df.shape[0]}', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the miles per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineer MILE_YEAR from ODOMETER\n",
    "df['MILE_YEAR'] = np.round(df['ODOMETER']/df['VEHICLE_AGE'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the new engineered features `MILE_YEAR` and `VEHICLE_AGE` we remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the outliers\n",
    "df = df[df.MILE_YEAR <= 40000]\n",
    "df = df[~((df.VEHICLE_AGE > 10) & (df.MILE_YEAR < 1000))]\n",
    "print(colored(f'\\nRecords after dropping rows where MILE_YEAR > 40,000: {df.shape[0]}', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the engine - weight ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineer ENGINE_WEIGHT_RATIO\n",
    "df['ENGINE_WEIGHT_RATIO'] = np.round(df['ENGINE_SIZE']/df['GVWR'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop remaining nan's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And select a subset of columns, which are most useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['MODEL_YEAR',\n",
    "        'VEHICLE_AGE',\n",
    "        'MILE_YEAR',\n",
    "        'MAKE',\n",
    "        'ENGINE_WEIGHT_RATIO',\n",
    "        'RESULT'\n",
    "        ]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid unnecessary many columns after one-hot encoding we create a make label `other` for all makes that only account for less than 1% of cars each and together approximately <10% of cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_threshold = 0.01\n",
    "#create a make label 'other' for all makes that only account for less than 1% of cars each and together aprox <10% of cars\n",
    "value_counts_norm = df['MAKE'].value_counts(normalize = True)\n",
    "to_other = value_counts_norm[value_counts_norm < make_threshold]\n",
    "#print(f\"\\n{len(to_other)} make labels each account for less than {round((make_threshold *100), 2)}% of cars and together account for {(round(to_other.sum(), 4)) *100}% of cars\")\n",
    "to_keep = value_counts_norm[value_counts_norm >= make_threshold]\n",
    "makes_keep = list(to_keep.index)\n",
    "makes_keep.sort()\n",
    "other_makes = [make for make in df['MAKE'].unique() if make not in makes_keep]\n",
    "df['MAKE'] = df['MAKE'].replace(other_makes, 'other')\n",
    "# one-hot encode the make column\n",
    "df = pd.get_dummies(df, drop_first=True, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into train-, validation- and test-sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a utility from sklearn to split and shuffle your dataset.\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('RESULT'))\n",
    "val_labels = np.array(val_df.pop('RESULT'))\n",
    "test_labels = np.array(test_df.pop('RESULT'))\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)\n",
    "\n",
    "print(f'Average class probability in training set:   {train_labels.mean():.4f}')\n",
    "print(f'Average class probability in validation set: {val_labels.mean():.4f}')\n",
    "print(f'Average class probability in test set:       {test_labels.mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are dealing with an highly imbalanced data set. We will use class weights to deal with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are highly sensitive to the scale of the input data, and small differences in scale can have a significant impact on the performance of the network. By properly scaling input data, we can ensure that the neural network is able to effectively learn and make accurate predictions. Therefore we add a normalizer layer which we adapt on the train set only, to avoid data leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point the steps should look familiar to you. But now we will create our very simple Dense Neural Network with: \n",
    "- an normalizer layer  \n",
    "- a dense layer with 4 nodes and gelu activation\n",
    "- a hidden layers with 4 nodes and gelu activation  \n",
    "- a dropout layer (The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.)\n",
    "- an output layer which is used to predict a car's test result. The output layer has a sigmoid activation function, which is used to 'squash' all our outputs to be between 0 and 1.\n",
    "\n",
    "We then compile our NN with different hyperparameters like:\n",
    "\n",
    "1. Optimizers:\n",
    "While the architecture of the Neural Network plays an important role when extracting information from data, all (most) are being optimized through update rules based on the gradient of the loss function.\n",
    "The update rules are determined by the Optimizer. The performance and update speed may heavily vary from optimizer to optimizer. The gradient tells us the update direction, but it is still unclear how big of a step we might take. Short steps keep us on track, but it might take a very long time until we reach a (local) minimum. Large steps speed up the process, but it might push us off the right direction.\n",
    "Adam and RMSProp are two very popular optimizers still being used in most neural networks. Both update the variables using an exponential decaying average of the gradient and its squared. But there are more, have a [look](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).\n",
    "\n",
    "2. Loss\n",
    "Different [loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    "\n",
    "3. Metrics\n",
    "\n",
    "4. and so on. There are many. Have a look yourself in the [documentary](https://www.tensorflow.org/api_docs/python/tf/keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(4, activation='gelu'),\n",
    "    tf.keras.layers.Dense(4, activation='gelu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=opt,\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            metrics=[tf.keras.metrics.Precision(thresholds=0.5), tf.keras.metrics.Recall(thresholds=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, \n",
    "    show_layer_names=True, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to predict failed test results, but we don't have very many of those positive samples to work with, so we would want to have the classifier heavily weight the few examples that are available. We can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class. Note, however, that this does not increase in any way the amount of information of your dataset. Let's see how it works out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "total = len(train_labels)\n",
    "pos = sum(train_labels)\n",
    "neg = total - pos\n",
    "\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=256,\n",
    "    epochs=8,\n",
    "    validation_data=(val_features, val_labels), \n",
    "    verbose=1,\n",
    "    class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at our recall at each epoch on the train- and validationset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for recall\n",
    "plt.plot(training.history['recall'])\n",
    "plt.plot(training.history['val_recall'])\n",
    "plt.title('model recall')\n",
    "plt.ylabel('recall')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise:** \n",
    "Plot the loss against the epochs, and discuss the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_features)\n",
    "\n",
    "# Plotting the confusing matrix\n",
    "mat = confusion_matrix(test_labels, y_pred.round())\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(test_labels, y_pred.round()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall tells us what fraction of the vehicles that will fail the emissions test, we actually catch. To reduce pollution in Albuquerque we want this to be as high as possible without having to test as few as possible vehicles who will pass the test. So that we save money."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise:** \n",
    "- What happens when you add layers to our model? \n",
    "- What happens when you change the number of nodes?\n",
    "- What when you change the batch size or the optimizer?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
