{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EIdT9iu_Z4Rb"
   },
   "source": [
    "# Basic regression: Predict fuel efficiency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AHp3M9ZmrIxj"
   },
   "source": [
    "*This notebook is based on the [tutorial notebook](https://www.tensorflow.org/tutorials/keras/regression) provided by TensorFlow.*\n",
    "\n",
    "---\n",
    "In a *regression* problem, we aim to predict the output of a continuous value, like a price or a probability. \n",
    "\n",
    "This notebook uses the classic [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) Dataset, which you've already encountered in previous notebooks, and builds a model to predict the fuel efficiency of late-1970s and early 1980s automobiles. To do this, we'll provide the model with a description of many automobiles from that time period. This description includes attributes like: cylinders, displacement, horsepower, and weight.\n",
    "\n",
    "This example uses the `tf.keras` API, see [this guide](https://www.tensorflow.org/guide/keras) for details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rRo8oNqZ-Rj"
   },
   "outputs": [],
   "source": [
    "import datetime, time, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xQKvCJ85kCQ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.layers as kl\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for TensorBoard\n",
    "\n",
    "We will use the TensorBoard to visualize some results. You can find more information and the board itself at the end of this notebook, but we will define the path were the information should be stored directly here at the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this command you can clear any logs from previous runs\n",
    "# If you want to compare different runs you can skip this cell \n",
    "!rm -rf my_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path for new directory \n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for creating a new folder for each run\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime('run_%d_%m_%Y-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for using callbacks; \"name\" should be the name of the model you use\n",
    "def get_callbacks(name):\n",
    "    return tf.keras.callbacks.TensorBoard(run_logdir+name, histogram_freq=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F_72b0LCNbjx"
   },
   "source": [
    "## The Auto MPG dataset\n",
    "\n",
    "The dataset is available from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gFh9ne3FZ-On"
   },
   "source": [
    "### Get the data\n",
    "First download and import the dataset using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiX2FI4gZtTt"
   },
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oY3pMPagJrO"
   },
   "outputs": [],
   "source": [
    "df = raw_dataset.copy()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3MWuJTKEDM-f"
   },
   "source": [
    "### Clean the data\n",
    "\n",
    "The `Horespower` column contains a few unknown values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEJHhN65a2VV"
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9UPN0KBHa_WI"
   },
   "source": [
    "Let's drop those rows to keep this tutorial simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZUDosChC1UN"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the datatypes and amount of unique values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8XKitwaH4v8h"
   },
   "source": [
    "The `\"Origin\"` column is really categorical, not numeric. So we have to convert that to a one-hot/dummy:\n",
    "\n",
    "(Note: You can set up the `keras.Model` to do this kind of transformation for you. That's beyond the scope of this tutorial. See the [preprocessing layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers) or [Loading CSV data](https://www.tensorflow.org/tutorials/load_data/csv) tutorials for examples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWNTD2QjBWFJ"
   },
   "outputs": [],
   "source": [
    "df['Origin'] = df['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulXz4J7PAUzk"
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, prefix='', prefix_sep='', dtype='uint8')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Cuym4yvk76vU"
   },
   "source": [
    "### Split the data into train and test\n",
    "\n",
    "Now we'll split the dataset into a training set and a test set.\n",
    "\n",
    "We will use the test set in the final evaluation of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qn-IGhUE7_1H"
   },
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.8, random_state=0)\n",
    "df_test = df.drop(df_train.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "J4ubs136WLNp"
   },
   "source": [
    "### Inspecting the data\n",
    "\n",
    "Let's have a quick look at the joint distribution of a few pairs of columns from the training set.\n",
    "\n",
    "Looking at the top row it should be clear that the fuel efficiency (MPG) is a function of all the other parameters. Looking at the other rows it should be clear that they are each functions of eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRKO_x8gWKv-"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_train[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gavKO_6DWRMP"
   },
   "source": [
    "Also look at the overall statistics, note how each feature covers a very different range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yi2FzC3T21jR"
   },
   "outputs": [],
   "source": [
    "df_train.describe().transpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Db7Auq1yXUvh"
   },
   "source": [
    "### Split features from labels\n",
    "\n",
    "Before we can start with the modelling process we need to separate our label from the dataset. This label is the value that we will train the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2sluJdCW7jN"
   },
   "outputs": [],
   "source": [
    "X_train = df_train.copy()\n",
    "X_test = df_test.copy()\n",
    "\n",
    "y_train = X_train.pop('MPG')\n",
    "y_test = X_test.pop('MPG')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn\n",
    "\n",
    "Before we start using TensorFlow and Keras, let's train a simple `LinearRegression` model from sklearn for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalizing and training the model \n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions \n",
    "y_pred = lin_reg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluting model\n",
    "mae =  mean_absolute_error(y_test, y_pred).round(2)\n",
    "mse = mean_squared_error(y_test, y_pred).round(2)\n",
    "\n",
    "print('MAE:', mae)\n",
    "print('MSE', mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll store the result in a dictionary in order to compare the results of different models in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "test_results['sklearn_model'] =  [mae, mse]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mRklxK5s388r"
   },
   "source": [
    "## Normalization\n",
    "\n",
    "In the table of statistics it's easy to see how different the ranges of each feature are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcmY6lKKbkw8"
   },
   "outputs": [],
   "source": [
    "X_train.describe().transpose()[['mean', 'std']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-ywmerQ6dSox"
   },
   "source": [
    "It is good practice to normalize features that use different scales and ranges. \n",
    "\n",
    "One reason this is important is because the features are multiplied by the model weights. So the scale of the outputs and the scale of the gradients are affected by the scale of the inputs. \n",
    "\n",
    "Although a model *might* converge without feature normalization, normalization makes training much more stable. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aFJ6ISropeoo"
   },
   "source": [
    "### The Normalization layer\n",
    "The `preprocessing.Normalization` layer is a clean and simple way to build that preprocessing into your model.\n",
    "\n",
    "The first step is to create the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlC5ooJrgjQF"
   },
   "outputs": [],
   "source": [
    "normalizer = kl.Normalization()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XYA2Ap6nVOha"
   },
   "source": [
    "Then `.adapt()` it to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrBbbjbwV91f"
   },
   "outputs": [],
   "source": [
    "normalizer.adapt(X_train.values) # adapt expect an array"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oZccMR5yV9YV"
   },
   "source": [
    "This calculates the mean and variance, and stores them in the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGn-ukwxSPtx"
   },
   "outputs": [],
   "source": [
    "normalizer.mean.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oGWKaF9GSRuN"
   },
   "source": [
    "When the layer is called it returns the input data, with each feature independently normalized. We can have a look at the first training instance and compare the original and normalized features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2l7zFL_XWIRu"
   },
   "outputs": [],
   "source": [
    "first = X_train[:1]*1.\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print('First example:', first)\n",
    "    print()\n",
    "    print('Normalized:', normalizer(first.values).numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6o3CrycBXA2s"
   },
   "source": [
    "## Linear regression\n",
    "\n",
    "Before building a DNN (deep neural network) model, let's start with a linear regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lFby9n0tnHkw"
   },
   "source": [
    "### One Variable\n",
    "\n",
    "We'll start easy with a single-variable linear regression, to predict `MPG` from `Horsepower`.\n",
    "\n",
    "Training a model with `tf.keras` typically starts by defining the model architecture.\n",
    "\n",
    "In this case we'll use a `keras.Sequential` model. This model represents a sequence of steps. In this case there are two steps:\n",
    "\n",
    "1. Normalize the input `horsepower`.\n",
    "2. Apply a linear transformation ($y = mx+b$) to produce 1 output using `layers.Dense`.\n",
    "\n",
    "The number of _inputs_ can either be set by the `input_shape` argument, or automatically when the model is run for the first time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp3gAFn3TPv8"
   },
   "source": [
    "1. First create the horsepower `Normalization` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl.Input(shape=[1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gJAy0fKs1TS"
   },
   "outputs": [],
   "source": [
    "horsepower = np.array(X_train['Horsepower']) # equivalent X_train['Horsepower'].values\n",
    "\n",
    "horsepower_normalizer = kl.Normalization(input_shape = [1,], axis= None)\n",
    "horsepower_normalizer.adapt(horsepower)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4NVlHJY2TWlC"
   },
   "source": [
    "2. Then we'll build the sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0sXM7qLlKfZ"
   },
   "outputs": [],
   "source": [
    "horsepower_model = tf.keras.Sequential([\n",
    "    horsepower_normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "# We can print a summary of the model architecture with the following line:\n",
    "horsepower_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eObQu9fDnXGL"
   },
   "source": [
    "This model will predict `MPG` from `Horsepower`.\n",
    "\n",
    "Run the untrained model on the first 10 horse-power values. The output won't be good, but you'll see that it has the expected shape, `(10,1)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horsepower_model.predict(horsepower[:10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfV1HS6bns-s"
   },
   "outputs": [],
   "source": [
    "horsepower_model.predict(horsepower[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CSkanJlmmFBX"
   },
   "source": [
    "Once the model is built, configure the training procedure using the `Model.compile()` method. The most important arguments to compile are the `loss` and the `optimizer` since these define what will be optimized (`mean_absolute_error`) and how (using the `optimizers.Adam`). We'll also define the `metrics` to use the `mean_squared_error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxA_3lpOm-SK"
   },
   "outputs": [],
   "source": [
    "horsepower_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    loss=['mae'],\n",
    "    metrics=['mse'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3q1I9TwnRSC"
   },
   "source": [
    "Once the training is configured, use `Model.fit()` to execute the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iSrNy59nRAp"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = horsepower_model.fit(\n",
    "    df_train['Horsepower'], y_train,\n",
    "    epochs=100,\n",
    "    # suppress logging (if you want to see the output for the different epochs set the value to 1)\n",
    "    verbose=0,\n",
    "    # Calculate validation results on 20% of the training data\n",
    "    validation_split = 0.2,\n",
    "    # Store information for TensorBoard\n",
    "    callbacks=get_callbacks(\"horsepower_model\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tQm3pc0FYPQB"
   },
   "source": [
    "Visualize the model's training progress using the stats stored in the `history` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCAwD_y4AdC3"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "# Show results from first 5 epochs\n",
    "hist.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results from last 5 epochs (loss and val_loss decreased)\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9E54UoZunqhc"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.ylim([0, 10])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [MPG]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYsQYrIZyqjz"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CMNrt8X2ebXd"
   },
   "source": [
    "We'll store the results on the test set for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDZ8EvNYrDtx"
   },
   "outputs": [],
   "source": [
    "test_results['horsepower_model'] = horsepower_model.evaluate(\n",
    "    X_test['Horsepower'],\n",
    "    y_test, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F0qutYAKwoda"
   },
   "source": [
    "SInce this is a single variable regression it's easy to look at the model's predictions as a function of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDS2JEtOn9Jn"
   },
   "outputs": [],
   "source": [
    "# We'll predict the MPG for 251 different values for horsepower \n",
    "x = tf.linspace(0.0, 250, 251)\n",
    "y = horsepower_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rttFCTU8czsI"
   },
   "outputs": [],
   "source": [
    "def plot_horsepower(x, y):\n",
    "    plt.scatter(X_train['Horsepower'], y_train, label='Data')\n",
    "    plt.plot(x, y, color='k', label='Predictions')\n",
    "    plt.xlabel('Horsepower')\n",
    "    plt.ylabel('MPG')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7l9ZiAOEUNBL"
   },
   "outputs": [],
   "source": [
    "plot_horsepower(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Yk2RmlqPoM9u"
   },
   "source": [
    "### Multiple inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PribnwDHUksC"
   },
   "source": [
    "You can use an almost identical setup to make predictions based on multiple inputs. This model still does the same $y = mx+b$ except that $m$ is a matrix and $b$ is a vector.\n",
    "\n",
    "This time we'll use the `Normalization` layer that was adapted to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssnVcKg7oMe6"
   },
   "outputs": [],
   "source": [
    "linear_model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IHlx6WeIWyAr"
   },
   "source": [
    "When you call this model on a batch of inputs, it produces `units=1` outputs for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DynfJV18WiuT"
   },
   "outputs": [],
   "source": [
    "linear_model.predict(X_train[:10]*1.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hvHKH3rPXHmq"
   },
   "source": [
    "When you call the model it's weight matrices will be built. Now you can see that the `kernel` (the $m$ in $y=mx+b$) has a shape of `(9,1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwJ4Fq0RXBQf"
   },
   "outputs": [],
   "source": [
    "linear_model.layers[1].kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eINAc6rZXzOt"
   },
   "source": [
    "Use the same `compile` and `fit` calls as for the single input `horsepower` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0Sv_Ybr0szp"
   },
   "outputs": [],
   "source": [
    "linear_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    loss=['mae'],\n",
    "    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZoOYORvoTSe"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = linear_model.fit(\n",
    "    X_train*1., y_train, \n",
    "    epochs=100,\n",
    "    # suppress logging (again: change it to 1 if you want to print more information about each epoch)\n",
    "    verbose=0,\n",
    "    # Calculate validation results on 20% of the training data\n",
    "    validation_split = 0.2,\n",
    "    # Store information for TensorBoard\n",
    "    callbacks=get_callbacks(\"linear_model\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EdxiCbiNYK2F"
   },
   "source": [
    "Using all the inputs achieves a much lower training and validation error than the `horsepower` model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sWO3W0koYgu"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NyN49hIWe_NH"
   },
   "source": [
    "We'll collect the test set result in our previously defined dictionary for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNC3D1DGsGgK"
   },
   "outputs": [],
   "source": [
    "test_results['linear_model'] = linear_model.evaluate(\n",
    "    np.array(X_test).astype('float32'), y_test, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SmjdzxKzEu1-"
   },
   "source": [
    "## A DNN regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DT_aHPsrzO1t"
   },
   "source": [
    "The previous section implemented linear models for single and multiple inputs.\n",
    "\n",
    "This section implements single-input and multiple-input DNN models. The code is basically the same except the model is expanded to include some \"hidden\"  non-linear layers. The name \"hidden\" here just means not directly connected to the inputs or outputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6SWtkIjhrZwa"
   },
   "source": [
    "These models will contain a few more layers than the linear model:\n",
    "\n",
    "1. The normalization layer.\n",
    "2. Two hidden, nonlinear, `Dense` layers using the `gelu` nonlinearity.\n",
    "3. A linear single-output layer.\n",
    "\n",
    "Both will use the same training procedure so we'll include the `compile` method in the newly defined `build_and_compile_model` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c26juK7ZG8j-"
   },
   "outputs": [],
   "source": [
    "def build_and_compile_model(norm):\n",
    "    model = keras.Sequential([\n",
    "        norm,\n",
    "        layers.Dense(64, activation='gelu'),\n",
    "        layers.Dense(64, activation='gelu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=['mae'],\n",
    "        metrics=['mse'],)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7T4RP1V36gVn"
   },
   "source": [
    "### One variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xvu9gtxTZR5V"
   },
   "source": [
    "Let's start again with a DNN model for a single input: \"Horsepower\".\n",
    "\n",
    "We can use the normalization layer we've created for our first model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGbPb-PHGbhs"
   },
   "outputs": [],
   "source": [
    "dnn_horsepower_model = build_and_compile_model(horsepower_normalizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Sj49Og4YGULr"
   },
   "source": [
    "This model has quite a few more trainable parameters than the linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReAD0n6MsFK-"
   },
   "outputs": [],
   "source": [
    "dnn_horsepower_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0-qWCsh6DlyH"
   },
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sD7qHCmNIOY0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = dnn_horsepower_model.fit(\n",
    "    X_train['Horsepower'], y_train,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100,\n",
    "    callbacks=get_callbacks(\"dnn_horespower\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dArGGxHxcKjN"
   },
   "source": [
    "This model does slightly better than the linear-horsepower model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcF6UWjdCU8T"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TG1snlpR2QCK"
   },
   "source": [
    "If you plot the predictions as a function of `Horsepower`, you'll see how this model takes advantage of the nonlinearity provided by the hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPF53Rem14NS"
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(0.0, 250, 251)\n",
    "y = dnn_horsepower_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsf9rD8I17Wq"
   },
   "outputs": [],
   "source": [
    "plot_horsepower(x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WxCJKIUpe4io"
   },
   "source": [
    "We'll also collect the results on the test set, for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJjM0dU52XtN"
   },
   "outputs": [],
   "source": [
    "test_results['dnn_horsepower_model'] = dnn_horsepower_model.evaluate(\n",
    "    X_train['Horsepower'], y_train,\n",
    "    verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "S_2Btebp2e64"
   },
   "source": [
    "### Full model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aKFtezDldLSf"
   },
   "source": [
    "If you repeat this process using all the inputs it slightly improves the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0mhscXh2k36"
   },
   "outputs": [],
   "source": [
    "dnn_model = build_and_compile_model(normalizer)\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXDENACl2tuW"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = dnn_model.fit(\n",
    "    X_train*1., y_train,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100,\n",
    "    callbacks=get_callbacks(\"dnn_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9Dbj0fX23RQ"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hWoVYS34fJPZ"
   },
   "source": [
    "Collect the results on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bZIa96W3c7K"
   },
   "outputs": [],
   "source": [
    "test_results['dnn_model'] = dnn_model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uiCucdPLfMkZ"
   },
   "source": [
    "## Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rDf1xebEfWBw"
   },
   "source": [
    "Now that all the models are trained let's check the test-set performance and compare how they performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5_ooufM5iH2"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_results, index=['Mean absolute error [MPG]', 'Mean squared error [MPG]']).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DABIVzsCf-QI"
   },
   "source": [
    "These results match the validation error seen during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ft603OzXuEZC"
   },
   "source": [
    "### Make predictions\n",
    "\n",
    "Finally, predict have a look at the errors made by the model when making predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xe7RXH3N3CWU"
   },
   "outputs": [],
   "source": [
    "y_pred = dnn_model.predict(X_test).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "19wyogbOSU5t"
   },
   "source": [
    "It looks like the model predicts reasonably well. \n",
    "\n",
    "Now take a look at the error distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-OHX4DiXd8x"
   },
   "outputs": [],
   "source": [
    "error = y_pred - y_test\n",
    "plt.hist(error, bins=25)\n",
    "plt.xlabel('Prediction Error [MPG]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KSyaHUfDT-mZ"
   },
   "source": [
    "If you're happy with the model you can save it for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-WwLlmfT-mb"
   },
   "outputs": [],
   "source": [
    "dnn_model.save('dnn_model.keras')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Benlnl8UT-me"
   },
   "source": [
    "If you reload the model, it will give you identical outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyyyj2zVT-mf"
   },
   "outputs": [],
   "source": [
    "reloaded = tf.keras.models.load_model('dnn_model.keras')\n",
    "\n",
    "test_results['reloaded'] = reloaded.evaluate(\n",
    "    X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_GchJ2tg-2o"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_results, index=['Mean absolute error [MPG]', 'Mean squared error [MPG]']).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vgGQuV-yqYZH"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook introduced a few techniques to handle a regression problem. Here are a few more tips that may help:\n",
    "\n",
    "* [Mean Squared Error (MSE)](https://www.tensorflow.org/api_docs/python/tf/losses/MeanSquaredError) and [Mean Absolute Error (MAE)](https://www.tensorflow.org/api_docs/python/tf/losses/MeanAbsoluteError) are common loss functions used for regression problems. Mean Absolute Error is less sensitive to outliers. Different loss functions are used for classification problems.\n",
    "* Similarly, evaluation metrics used for regression differ from classification.\n",
    "* When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n",
    "* Overfitting is a common problem for DNN models, it wasn't a problem for this tutorial. See the [overfit and underfit](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit) tutorial for more help with this.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TensorBoard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, to improve something you often need to be able to measure it. TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more.\n",
    "\n",
    "TensorBoard can be used directly within a notebook in Colab and Jupyter. This can be helpful for sharing results, integrating TensorBoard into existing workflows, and using TensorBoard without installing anything locally.\n",
    "\n",
    "You can find a nice introductory notebook [here](https://www.tensorflow.org/tensorboard/get_started).\n",
    "\n",
    "When you are running this NB in a browster and the tensorboard cannot be displayed: If you are using Safari, try to switch to Google Chrome and run it again."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using TensorBoard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training with Keras's `.fit()`, adding the `tf.keras.callbacks.TensorBoard` callback ensures that logs are created and stored. Additionally, enable histogram computation every epoch with histogram_freq=1 (this is turned off by default).\n",
    "\n",
    "We've saved the logs in a timestamped subdirectory to allow easy selection of different training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start TensorBoard within the notebook using magics. At the end of the command you need to specify the path where the log files are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./my_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "regression.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
