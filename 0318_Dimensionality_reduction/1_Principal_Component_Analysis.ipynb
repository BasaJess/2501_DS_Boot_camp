{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "## Content of this notebook\n",
    "1. Brief intro into PCA using the wine-quality dataset. \n",
    "2. we will explore the data set a bit. Interesting to look at is the correlation between the variables.\n",
    "3. we will check how well features can separate the 3 Classes of wine (graphically).\n",
    "4. we will scale our data and apply PCA.\n",
    "5. we will check how much information is stored in each newly created principal component and check how well the first 2 principal components can separate the 3 Classes of wine.\n",
    "6. Math behind PCA (optional)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Brief primer and history\n",
    "\n",
    "PCA was invented in 1901 by [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson) as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by [Harold Hotelling](https://en.wikipedia.org/wiki/Harold_Hotelling) in the 1930s.\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an [orthogonal transformation](https://en.wikipedia.org/wiki/Orthogonal_transformation) to convert a set of observations of possibly correlated variables into a set of values of [linearly uncorrelated](https://en.wikipedia.org/wiki/Correlation_and_dependence) variables called principal components. \n",
    "\n",
    "The number of distinct principal components is equal to the smaller of the number of original variables or the number of observations minus one. \n",
    "\n",
    "This transformation is defined in such a way that the first principal component has the largest possible [variance](https://en.wikipedia.org/wiki/Variance) (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is [orthogonal](https://en.wikipedia.org/wiki/Orthogonal) the preceding components. The resulting vectors are an uncorrelated [orthogonal basis set](https://en.wikipedia.org/wiki/Orthogonal_basis_set). \n",
    "\n",
    "Keep in mind that PCA is sensitive to the relative scaling of the original variables!\n",
    "\n",
    "If you are interested in the math behind PCA, you will find a section about it at the end of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:36.099320Z",
     "start_time": "2020-07-02T10:00:34.959220Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data and perform basic exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:36.156431Z",
     "start_time": "2020-07-02T10:00:36.101573Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wine_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:36.272138Z",
     "start_time": "2020-07-02T10:00:36.160139Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[:,1:].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots by output labels/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:40.330720Z",
     "start_time": "2020-07-02T10:00:36.281647Z"
    }
   },
   "outputs": [],
   "source": [
    "for c in df.columns[1:]:\n",
    "    df.boxplot(c,by='Class',figsize=(7,4),fontsize=14)\n",
    "    plt.title(\"{}\\n\".format(c),fontsize=16)\n",
    "    plt.xlabel(\"Wine Class\", fontsize=16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It can be seen that some features classify the wine labels pretty clearly.** For example, Alcalinity, Total Phenols, or Flavonoids produce box plots with well-separated medians, which are clearly indicative of wine classes.\n",
    "\n",
    "Below is an example of class separation using two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:40.746597Z",
     "start_time": "2020-07-02T10:00:40.334432Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "scatter = plt.scatter(df['OD280/OD315 of diluted wines'],df['Flavanoids'],c=df['Class'],edgecolors='k',alpha=0.75,s=150)\n",
    "plt.grid(True)\n",
    "classes = ['1', '2', '3']\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
    "plt.title(\"Scatter plot of two features showing the \\ncorrelation and class separation\",fontsize=15)\n",
    "plt.xlabel(\"OD280/OD315 of diluted wines\",fontsize=15)\n",
    "plt.ylabel(\"Flavanoids\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the features independent? Plot co-variance matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:41.487228Z",
     "start_time": "2020-07-02T10:00:40.754925Z"
    }
   },
   "outputs": [],
   "source": [
    "def correlation_matrix(df):\n",
    "    from matplotlib import pyplot as plt\n",
    "    from matplotlib import cm as cm\n",
    "\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    cmap = cm.get_cmap('jet', 30)\n",
    "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
    "    ax1.grid(True)\n",
    "    plt.title('Wine data set features correlation\\n',fontsize=15)\n",
    "    labels=df.columns\n",
    "    ax1.set_xticks(np.arange(14))\n",
    "    ax1.set_xticklabels(labels,fontsize=11, rotation=90)\n",
    "    ax1.set_yticks(np.arange(14))\n",
    "    ax1.set_yticklabels(labels,fontsize=11)\n",
    "    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
    "    fig.colorbar(cax, ticks=[0.1*i for i in range(-11,11)])\n",
    "    plt.show()\n",
    "\n",
    "correlation_matrix(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there are some good amount of correlation between features i.e. they are not independent of each other. Independence of variables is a typical preassumption of algorithms (eg.in Naive Bayes). However, we will still go ahead and apply the classifier to see its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.120493Z",
     "start_time": "2020-07-02T10:00:42.108925Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('Class',axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scaling\n",
    "PCA requires scaling/normalization of the data to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.087807Z",
     "start_time": "2020-07-02T10:00:41.492059Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.103931Z",
     "start_time": "2020-07-02T10:00:42.093751Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.143246Z",
     "start_time": "2020-07-02T10:00:42.127959Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.160826Z",
     "start_time": "2020-07-02T10:00:42.146496Z"
    }
   },
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(data=X_train_scaled,columns=df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.229087Z",
     "start_time": "2020-07-02T10:00:42.169243Z"
    }
   },
   "outputs": [],
   "source": [
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.396337Z",
     "start_time": "2020-07-02T10:00:42.239511Z"
    }
   },
   "outputs": [],
   "source": [
    "df_scaled.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA class import and analysis\n",
    "\n",
    "[PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) is also already implemented in scikit-learn. Check out the parameters that can be set for PCA and the attributes that are calculated after PCA is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.494590Z",
     "start_time": "2020-07-02T10:00:42.400774Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.504749Z",
     "start_time": "2020-07-02T10:00:42.499187Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.534015Z",
     "start_time": "2020-07-02T10:00:42.510834Z"
    }
   },
   "outputs": [],
   "source": [
    "df_scaled_pca = pca.fit(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try out some attributes of pca and check your understanding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the _explained variance ratio_ for each principal component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.925221Z",
     "start_time": "2020-07-02T10:00:42.543081Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x=[i+1 for i in range(len(df_scaled_pca.explained_variance_ratio_))],\n",
    "            y=df_scaled_pca.explained_variance_ratio_,\n",
    "            s=200, alpha=0.75,c='orange',edgecolor='k')\n",
    "plt.grid(True)\n",
    "plt.title(\"Explained variance ratio of the \\nfitted principal component vector\\n\",fontsize=25)\n",
    "plt.xlabel(\"Principal components\",fontsize=15)\n",
    "plt.xticks([i+1 for i in range(len(df_scaled_pca.explained_variance_ratio_))],fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylabel(\"Explained variance ratio\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot means that the **first principal component explains about 36%** of the total variance in the data and the **second component explians further 20%**. Therefore, if we just consider first two components, they together explain **56%** of the total variance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing better class separation using principal components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the scaled data set using the fitted PCA object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:42.939147Z",
     "start_time": "2020-07-02T10:00:42.932939Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled_trans = pca.transform(df_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put it in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:43.002952Z",
     "start_time": "2020-07-02T10:00:42.942497Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled_trans = pd.DataFrame(data=X_train_scaled_trans)\n",
    "X_train_scaled_trans.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the first two columns of this transformed data set with the color set to original ground truth class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-02T10:00:43.473748Z",
     "start_time": "2020-07-02T10:00:43.032640Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X_train_scaled_trans[0],X_train_scaled_trans[1],c=y_train,edgecolors='k',alpha=0.75,s=150)\n",
    "classes = ['1', '2', '3']\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=classes)\n",
    "plt.grid(True)\n",
    "plt.title(\"Class separation using first two principal components\\n\",fontsize=20)\n",
    "plt.xlabel(\"Principal component-1\",fontsize=15)\n",
    "plt.ylabel(\"Principal component-2\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically it's clear that the first 2 principal components can separate the classes better than the 2 most correlated variables with the target variable.\n",
    "Let's see if this intuition of the graphs holds true when using a model to predict the Class of a wine.\n",
    "Because we mentioned the naive bayes before, let's test it using this classifier. \n",
    "\n",
    "If you are not familiar with the naive bayes classifier - that is not a problem. You can learn more about it on [scikit-learn.org](https://scikit-learn.org/stable/modules/naive_bayes.html) but for this notebook it doesn't really matter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"Flavanoids\", \"OD280/OD315 of diluted wines\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: instantiate the model and train it on X_train, y_train (data without any transformations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: predict the classes with the model on X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the accuracy score for an easy comparison of results\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how well the model deals when using the first and second principal component.\n",
    "Remember how the transformed data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_trans.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first and second component are stored in column 0 and 1\n",
    "variables = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: instantiate the model and train it on X_train_scaled_trans with the variables defined before, y_train (data any transformations)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can predict on the test data, we need to transform it first. \n",
    "Remember, we used standard scaler and PCA to transform our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use implemented standard scaler to scale data\n",
    "\n",
    "#TODO: transform the data with implemented PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_trans = pd.DataFrame(data=X_test_scaled_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: predict y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: calculate the accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are your conclusions with these classifications?\n",
    "- Which variables yield better results?\n",
    "\n",
    "Feel free to experiment further...\n",
    "For example \n",
    "- test a different classifier or \n",
    "- add more variables to your model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mathematical details\n",
    "PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n",
    "\n",
    "Consider a data matrix, $\\mathbf{X}$, with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), where each of the $n$ rows represents a different repetition of the experiment, and each of the $p$ columns gives a particular kind of feature (say, the results from a particular sensor).\n",
    "\n",
    "Mathematically, the transformation is defined by a set of p-dimensional vectors of weights or loadings\n",
    "${\\displaystyle \\mathbf {w} _{(k)}=(w_{1},\\dots ,w_{p})_{(k)}} \\mathbf {w} _{(k)}=(w_{1},\\dots ,w_{p})_{(k)}$ that map each row vector ${\\displaystyle \\mathbf {x} _{(i)}} \\mathbf{x}_{(i)}$ of $\\mathbf{X}$ to a new vector of principal component scores ${\\displaystyle \\mathbf {t} _{(i)}=(t_{1},\\dots ,t_{m})_{(i)}}$ given by\n",
    "\n",
    "$${\\displaystyle {t_{k}}_{(i)}=\\mathbf {x} _{(i)}\\cdot \\mathbf {w} _{(k)}\\qquad \\mathrm {for} \\qquad i=1,\\dots ,n\\qquad k=1,\\dots ,m} {\\displaystyle {t_{k}}_{(i)}=\\mathbf {x} _{(i)}\\cdot \\mathbf {w} _{(k)}\\qquad \\mathrm {for} \\qquad i=1,\\dots ,n\\qquad k=1,\\dots ,m}$$\n",
    "\n",
    "in such a way that the individual variables ${\\displaystyle t_{1},\\dots ,t_{m}}$ of t considered over the data set successively inherit the maximum possible variance from $\\mathbf{x}$, with each loading vector $\\mathbf{w}$ constrained to be a unit vector.\n",
    "\n",
    "In order to maximize variance, the first loading vector $\\mathbf {w} _{(1)}$ thus has to satisfy\n",
    "\n",
    "$$ {\\displaystyle \\mathbf {w} _{(1)}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\left\\{\\sum _{i}\\left(t_{1}\\right)_{(i)}^{2}\\right\\}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\left\\{\\sum _{i}\\left(\\mathbf {x} _{(i)}\\cdot \\mathbf {w} \\right)^{2}\\right\\}}$$\n",
    "\n",
    "Equivalently, writing this in matrix form gives\n",
    "\n",
    "$${\\displaystyle \\mathbf {w} _{(1)}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\{\\Vert \\mathbf {Xw} \\Vert ^{2}\\}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\left\\{\\mathbf {w} ^{T}\\mathbf {X} ^{T}\\mathbf {Xw} \\right\\}}$$\n",
    "\n",
    "Since $\\mathbf {w} _{(1)}$ has been defined to be a unit vector, it equivalently also satisfies\n",
    "$${\\displaystyle \\mathbf {w} _{(1)}={\\operatorname {\\arg \\,max} }\\,\\left\\{{\\frac {\\mathbf {w} ^{T}\\mathbf {X} ^{T}\\mathbf {Xw} }{\\mathbf {w} ^{T}\\mathbf {w} }}\\right\\}}$$\n",
    "\n",
    "With $\\mathbf {w} _{(1)}$ found, the first principal component of a data vector $\\mathbf {x} _{(i)}$ can then be given as a score $\\mathbf {t} _{(i)}$ = $\\mathbf {x} _{(i)}$ ⋅ $\\mathbf {w} _{(1)}$ in the transformed co-ordinates, or as the corresponding vector in the original variables, {$\\mathbf {x} _{(i)}$ ⋅ $\\mathbf {w} _{(1)}$} $\\mathbf {w} _{(1)}$.\n",
    "\n",
    "The $k^{th}$ component can be found by subtracting the first $k$ − 1 principal components from $\\mathbf{X}$:\n",
    "\n",
    "$${\\displaystyle \\mathbf {\\hat {X}} _{k}=\\mathbf {X} -\\sum _{s=1}^{k-1}\\mathbf {X} \\mathbf {w} _{(s)}\\mathbf {w} _{(s)}^{\\rm {T}}}$$\n",
    "and then finding the loading vector which extracts the maximum variance from this new data matrix\n",
    "\n",
    "$${\\displaystyle \\mathbf {w} _{(k)}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {arg\\,max} }}\\left\\{\\Vert \\mathbf {\\hat {X}} _{k}\\mathbf {w} \\Vert ^{2}\\right\\}={\\operatorname {\\arg \\,max} }\\,\\left\\{{\\tfrac {\\mathbf {w} ^{T}\\mathbf {\\hat {X}} _{k}^{T}\\mathbf {\\hat {X}} _{k}\\mathbf {w} }{\\mathbf {w} ^{T}\\mathbf {w} }}\\right\\}}$$\n",
    "\n",
    "Computing the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) is now the standard way to calculate a principal components analysis from a data matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "180.208px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
